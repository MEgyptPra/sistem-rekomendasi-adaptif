#!/usr/bin/env python3
"""
Comprehensive test untuk Contextual Multi-Armed Bandit system
Tests the complete pipeline: Context Generation â†’ Contextual MAB â†’ MMR â†’ Recommendations
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.real_time_data import RealTimeContextService
from app.services.mab_optimizer import MABOptimizer
import time
import random

def test_context_generation():
    """Test RealTimeContextService"""
    print("=== Testing Context Generation ===")
    
    context_service = RealTimeContextService()
    
    # Generate multiple contexts to see variety
    print("ðŸŒ Generating sample contexts:")
    for i in range(5):
        context = context_service.get_current_context()
        print(f"   Context {i+1}: weather={context['weather']}, weekend={context['is_weekend']}, "
              f"hour={context['hour_of_day']}, season={context['season']}")
        time.sleep(0.1)  # Small delay to see time-based changes
    
    print("âœ… Context generation working correctly\n")
    return context_service

def test_contextual_mab():
    """Test Contextual MAB functionality"""
    print("=== Testing Contextual MAB ===")
    
    mab = MABOptimizer(n_arms=11, exploration_param=2.0)
    context_service = RealTimeContextService()
    
    # Simulate different contexts and their preferences
    contexts_and_preferences = [
        # Context 1: Sunny weather prefers more relevance (higher lambda)
        ({"weather": "cerah", "is_weekend": False, "hour_of_day": 14, "season": "summer"}, 
         [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.85, 0.7, 0.6]),  # Prefer Î»=0.7-0.8
        
        # Context 2: Rainy weather prefers more diversity (lower lambda)  
        ({"weather": "hujan", "is_weekend": True, "hour_of_day": 10, "season": "winter"}, 
         [0.9, 0.8, 0.85, 0.7, 0.6, 0.5, 0.4, 0.3, 0.3, 0.2, 0.1]),  # Prefer Î»=0.1-0.3
        
        # Context 3: Weekend prefers balanced approach
        ({"weather": "berawan", "is_weekend": True, "hour_of_day": 16, "season": "spring"},
         [0.3, 0.4, 0.5, 0.6, 0.7, 0.85, 0.8, 0.7, 0.6, 0.5, 0.4])   # Prefer Î»=0.5
    ]
    
    print("ðŸ§  Training Contextual MAB with different context preferences:")
    
    # Training phase
    for round_num in range(60):  # 60 rounds total, 20 per context
        context_idx = round_num % 3
        context, true_rewards = contexts_and_preferences[context_idx]
        
        # MAB selects arm based on context
        selected_arm = mab.select_arm(context)
        
        # Generate reward based on context preference + noise
        true_reward = true_rewards[selected_arm]
        noise = random.gauss(0, 0.1)  # Some noise
        observed_reward = max(0, min(1, true_reward + noise))
        
        # Update MAB with contextual feedback
        mab.update_reward(selected_arm, observed_reward, context)
        
        if round_num % 20 == 19:  # Every 20 rounds
            print(f"   Round {round_num+1}: Training context {context_idx+1}")
    
    print("\nðŸ“Š Contextual MAB Learning Results:")
    stats = mab.get_statistics()
    
    print(f"   Total contexts learned: {stats['total_contexts']}")
    print(f"   Total pulls across all contexts: {stats['total_pulls']}")
    
    print("\nðŸŽ¯ Best strategies per context:")
    for context_key, context_stats in stats["context_breakdown"].items():
        if context_stats["best_arm"] is not None:
            print(f"   {context_key}")
            print(f"      â†’ Best Î»: {context_stats['best_lambda']:.1f} "
                  f"(avg reward: {context_stats['best_avg_reward']:.3f})")
    
    # Test prediction for each context
    print("\nðŸ”® Testing predictions for each context:")
    for i, (context, _) in enumerate(contexts_and_preferences):
        predicted_arm = mab.select_arm(context)
        predicted_lambda = mab.get_lambda_value(predicted_arm)
        print(f"   Context {i+1} ({context['weather']}, weekend={context['is_weekend']}): "
              f"â†’ Î»={predicted_lambda:.1f}")
    
    print("âœ… Contextual MAB learning and prediction working correctly\n")
    return mab

def test_complete_pipeline():
    """Test the complete recommendation pipeline"""
    print("=== Testing Complete Pipeline Integration ===")
    
    context_service = RealTimeContextService()
    mab = MABOptimizer(n_arms=11, exploration_param=2.0)
    
    print("ðŸš€ Simulating complete recommendation flow:")
    
    # Simulate 10 recommendation requests
    for request_num in range(10):
        print(f"\nðŸ“± Recommendation Request #{request_num + 1}")
        
        # Step 1: Get current context
        current_context = context_service.get_current_context()
        print(f"   Context: {current_context['weather']} weather, "
              f"{'weekend' if current_context['is_weekend'] else 'weekday'}, "
              f"{current_context['hour_of_day']}:00")
        
        # Step 2: MAB selects lambda based on context
        selected_arm = mab.select_arm(current_context)
        selected_lambda = mab.get_lambda_value(selected_arm)
        print(f"   MAB Decision: Î»={selected_lambda:.1f} (arm {selected_arm})")
        
        # Step 3: Simulate MMR with selected lambda
        print(f"   MMR: Using Î»={selected_lambda:.1f} for relevance-diversity balance")
        if selected_lambda > 0.7:
            print(f"   â†’ Strategy: High relevance focus")
        elif selected_lambda < 0.3:
            print(f"   â†’ Strategy: High diversity focus")
        else:
            print(f"   â†’ Strategy: Balanced approach")
        
        # Step 4: Simulate user feedback
        # Assume some contexts work better with certain lambdas
        if current_context['weather'] == 'hujan' and selected_lambda < 0.5:
            simulated_reward = random.uniform(0.7, 0.9)  # Good for rainy weather
        elif current_context['weather'] == 'cerah' and 0.6 <= selected_lambda <= 0.8:
            simulated_reward = random.uniform(0.8, 1.0)  # Good for sunny weather
        else:
            simulated_reward = random.uniform(0.4, 0.7)  # Average performance
        
        # Step 5: Update MAB with feedback
        mab.update_reward(selected_arm, simulated_reward, current_context)
        print(f"   Feedback: {simulated_reward:.3f} reward â†’ MAB learns")
        
        time.sleep(0.2)  # Small delay between requests
    
    print("\nðŸ“ˆ Final Learning Summary:")
    final_stats = mab.get_statistics()
    print(f"   Contexts discovered: {final_stats['total_contexts']}")
    print(f"   Total learning experiences: {final_stats['total_pulls']}")
    
    best_contexts = [(k, v) for k, v in final_stats["context_breakdown"].items() 
                    if v["best_arm"] is not None]
    
    if best_contexts:
        print(f"   Top learned strategies:")
        for context_key, stats in sorted(best_contexts, 
                                       key=lambda x: x[1]["best_avg_reward"], 
                                       reverse=True)[:3]:
            print(f"      {context_key} â†’ Î»={stats['best_lambda']:.1f} "
                  f"(reward: {stats['best_avg_reward']:.3f})")
    
    print("âœ… Complete pipeline working correctly!")

def test_edge_cases():
    """Test edge cases and error handling"""
    print("\n=== Testing Edge Cases ===")
    
    mab = MABOptimizer(n_arms=11)
    
    # Test with None context
    print("ðŸ” Testing None context:")
    arm = mab.select_arm(None)
    print(f"   Selected arm {arm} when context=None")
    
    # Test with incomplete context
    print("ðŸ” Testing incomplete context:")
    incomplete_context = {"weather": "cerah"}  # Missing other fields
    arm = mab.select_arm(incomplete_context)
    print(f"   Selected arm {arm} with incomplete context")
    
    # Test invalid arm update
    print("ðŸ” Testing invalid feedback:")
    result = mab.update_reward(-1, 0.5, {"weather": "cerah"})  # Invalid arm
    print(f"   Handled invalid arm gracefully")
    
    print("âœ… Edge cases handled correctly")

if __name__ == "__main__":
    print("ðŸ§ª COMPREHENSIVE CONTEXTUAL MAB TESTING")
    print("="*60)
    
    # Run all tests
    context_service = test_context_generation()
    mab = test_contextual_mab()
    test_complete_pipeline()
    test_edge_cases()
    
    print("\n" + "="*60)
    print("ðŸŽ‰ ALL TESTS PASSED!")
    print("\nðŸ’¡ Key Achievements:")
    print("   âœ… Context generation with realistic time-based simulation")
    print("   âœ… Contextual MAB learning different strategies per context")
    print("   âœ… Complete pipeline integration: Context â†’ MAB â†’ MMR â†’ Feedback")
    print("   âœ… Robust error handling for edge cases")
    
    print("\nðŸš€ System Ready For:")
    print("   ðŸ“± Real user interactions")
    print("   ðŸ”„ Continuous learning from feedback")
    print("   ðŸ“Š Performance monitoring and analytics")
    print("   ðŸŽ¯ Context-aware adaptive recommendations")
    
    print("\nðŸ”® Next Implementation Steps:")
    print("   1. Integrate real-time API data sources")
    print("   2. Implement user feedback collection UI")
    print("   3. Add A/B testing framework")
    print("   4. Create monitoring dashboard")
    print("   5. Deploy to production environment")
