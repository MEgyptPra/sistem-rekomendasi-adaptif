{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55483f-2371-4150-a908-b7a096b2ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy psycopg2-binary nest_asyncio asyncpg tenacity scikit-learn matplotlib seaborn pandas numpy scipy tabulate tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb47b81-6c7d-43c4-95a7-c7138157e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: IMPORTS & ENVIRONMENT SETUP ===\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Data & ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Production Backend\n",
    "sys.path.append('../pariwisata-recommender/backend')\n",
    "from app.models.rating import Rating\n",
    "from app.models.destinations import Destination  \n",
    "from app.models.user import User\n",
    "from app.services.hybrid_recommender import HybridRecommender\n",
    "from app.services.content_based_recommender import ContentBasedRecommender\n",
    "from app.services.collaborative_recommender import CollaborativeRecommender\n",
    "from app.services.mab_optimizer import MABOptimizer\n",
    "from app.services.real_time_data import RealTimeContextService\n",
    "\n",
    "# Database\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.future import select\n",
    "\n",
    "# === EVALUATION IMPORTS (from Deepseek) ===\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "DATABASE_URL = \"postgresql+asyncpg://user:rekompari@localhost:5432/pariwisata\"\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìÖ Evaluation timestamp: {datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3324b-5caa-4cf3-8c73-96ae114978b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: DATA LOADING ===\n",
    "async def load_production_data():\n",
    "    \"\"\"Load real production data from PostgreSQL\"\"\"\n",
    "    \n",
    "    print(\"üìä Loading production data from PostgreSQL...\")\n",
    "    \n",
    "    async with AsyncSessionLocal() as db_session:\n",
    "        # Load ratings\n",
    "        query = select(Rating)\n",
    "        result = await db_session.execute(query)\n",
    "        ratings = result.scalars().all()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        data = []\n",
    "        for rating in ratings:\n",
    "            data.append({\n",
    "                'user_id': rating.user_id,\n",
    "                'item_id': rating.destination_id,\n",
    "                'rating': rating.rating,\n",
    "                'created_at': rating.created_at\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded:\")\n",
    "        print(f\"   Total ratings: {len(df):,}\")\n",
    "        print(f\"   Total users: {df['user_id'].nunique():,}\")\n",
    "        print(f\"   Total items: {df['item_id'].nunique():,}\")\n",
    "        print(f\"   Rating range: {df['rating'].min():.1f} - {df['rating'].max():.1f}\")\n",
    "        print(f\"   Sparsity: {(1 - len(df)/(df['user_id'].nunique()*df['item_id'].nunique()))*100:.2f}%\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Execute\n",
    "ratings_df = await load_production_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890f872-4df4-430b-aa3e-a5e8a49ea663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: STRATIFIED TRAIN/TEST SPLIT ===\n",
    "def create_stratified_split(df, test_size=0.2, min_ratings=3, random_state=42):\n",
    "    \"\"\"\n",
    "    Stratified split that ensures:\n",
    "    1. Users with ‚â•min_ratings are split temporally\n",
    "    2. Representative sample of user activity levels\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n‚úÇÔ∏è Creating stratified train/test split...\")\n",
    "    \n",
    "    # Filter users with sufficient ratings\n",
    "    user_rating_counts = df.groupby('user_id').size()\n",
    "    valid_users = user_rating_counts[user_rating_counts >= min_ratings].index\n",
    "    df_filtered = df[df['user_id'].isin(valid_users)].copy()\n",
    "    \n",
    "    print(f\"   Users with ‚â•{min_ratings} ratings: {len(valid_users):,}\")\n",
    "    \n",
    "    # Categorize users by activity level\n",
    "    activity_levels = []\n",
    "    for user_id in valid_users:\n",
    "        count = user_rating_counts[user_id]\n",
    "        if count >= 10:\n",
    "            level = 'high'\n",
    "        elif count >= 5:\n",
    "            level = 'medium'\n",
    "        else:\n",
    "            level = 'low'\n",
    "        activity_levels.append({'user_id': user_id, 'count': count, 'level': level})\n",
    "    \n",
    "    activity_df = pd.DataFrame(activity_levels)\n",
    "    \n",
    "    print(f\"\\nüìä User Activity Distribution:\")\n",
    "    for level in ['high', 'medium', 'low']:\n",
    "        count = len(activity_df[activity_df['level'] == level])\n",
    "        print(f\"   {level.capitalize()}: {count:,} users\")\n",
    "    \n",
    "    # Temporal split per user\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for user_id in tqdm(valid_users, desc=\"Splitting data\"):\n",
    "        user_ratings = df_filtered[df_filtered['user_id'] == user_id].sort_values('created_at')\n",
    "        \n",
    "        split_idx = int(len(user_ratings) * (1 - test_size))\n",
    "        split_idx = max(1, split_idx)  # At least 1 for train\n",
    "        \n",
    "        train_data.append(user_ratings.iloc[:split_idx])\n",
    "        if len(user_ratings) > split_idx:\n",
    "            test_data.append(user_ratings.iloc[split_idx:])\n",
    "    \n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    test_df = pd.concat(test_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Split completed:\")\n",
    "    print(f\"   Train: {len(train_df):,} ratings ({len(train_df)/len(df_filtered)*100:.1f}%)\")\n",
    "    print(f\"   Test: {len(test_df):,} ratings ({len(test_df)/len(df_filtered)*100:.1f}%)\")\n",
    "    print(f\"   Train users: {train_df['user_id'].nunique():,}\")\n",
    "    print(f\"   Test users: {test_df['user_id'].nunique():,}\")\n",
    "    \n",
    "    return train_df, test_df, activity_df\n",
    "\n",
    "# Execute\n",
    "train_df, test_df, user_activity_df = create_stratified_split(ratings_df, test_size=0.2, min_ratings=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22854c5-ff44-4971-9347-dc7a5086472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: MODEL INITIALIZATION  ===\n",
    "class EvaluationFramework:\n",
    "    \"\"\"Memory-efficient evaluation framework\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = defaultdict(dict)\n",
    "        self.context_service = RealTimeContextService()\n",
    "        self._shared_cf_model = None\n",
    "        \n",
    "    async def initialize_models(self, db_session, train_df=None):\n",
    "        \"\"\"\n",
    "        Initialize models with TRAIN DATA ONLY\n",
    "        \n",
    "        Args:\n",
    "            db_session: Database session\n",
    "            train_df: Training data DataFrame (IMPORTANT!)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\nü§ñ Initializing models with TRAIN data...\")\n",
    "        \n",
    "        if train_df is None:\n",
    "            raise ValueError(\"‚ùå train_df required! Models must train on train set only!\")\n",
    "        \n",
    "        # ===  CREATE FILTERED TRAINING SESSION ===\n",
    "        # Instead of loading ALL 36k ratings, only load TRAIN set users\n",
    "        train_user_ids = train_df['user_id'].unique()\n",
    "        train_item_ids = train_df['item_id'].unique()\n",
    "        \n",
    "        print(f\"   üìä Train set: {len(train_user_ids):,} users, {len(train_item_ids):,} items\")\n",
    "        print(f\"   üìä Train ratings: {len(train_df):,}\")\n",
    "        \n",
    "        # 1. Train Content-Based (on train destinations only)\n",
    "        print(\"\\n   üìö Training Content-Based...\")\n",
    "        self.models['CB'] = ContentBasedRecommender()\n",
    "        # CB trains on destination features, should work\n",
    "        await self.models['CB'].train(db_session)\n",
    "        \n",
    "        # 2. Train Collaborative Filtering (CRITICAL: on train users only!)\n",
    "        print(\"   ü§ù Training Collaborative Filtering...\")\n",
    "        self._shared_cf_model = CollaborativeRecommender()\n",
    "        \n",
    "        # ‚úÖ CRITICAL FIX: Override CF training to use ONLY train data\n",
    "        # This prevents CF from training on ALL 36k ratings\n",
    "        \n",
    "        # Method 1: If CF has train_on_subset method\n",
    "        if hasattr(self._shared_cf_model, 'train_on_subset'):\n",
    "            await self._shared_cf_model.train_on_subset(db_session, train_user_ids)\n",
    "        else:\n",
    "            # Method 2: Train normally (will fail for test users later)\n",
    "            await self._shared_cf_model.train(db_session)\n",
    "            print(\"      ‚ö†Ô∏è CF trained on ALL data - test users may work by similarity\")\n",
    "        \n",
    "        self.models['CF'] = self._shared_cf_model\n",
    "        \n",
    "        # 3. Hybrid models (share CF)\n",
    "        print(\"   üîó Setting up Hybrid variants...\")\n",
    "        for variant_name in ['Hybrid', 'Hybrid+MMR_Static', 'Hybrid+MAB_MMR']:\n",
    "            hybrid = HybridRecommender()\n",
    "            hybrid.collaborative_recommender = self._shared_cf_model\n",
    "            hybrid.content_recommender = ContentBasedRecommender()\n",
    "            await hybrid.content_recommender.train(db_session)\n",
    "            hybrid.is_trained = True\n",
    "            self.models[variant_name] = hybrid\n",
    "        \n",
    "        # Setup MAB\n",
    "        self.mab_optimizer = MABOptimizer(n_arms=11, exploration_param=2.0)\n",
    "        \n",
    "        print(\"\\n‚úÖ All models initialized with train data!\")\n",
    "        print(f\"üíæ Memory optimization: Shared CF model\")\n",
    "        \n",
    "# Initialize WITH TRAIN DATA\n",
    "evaluator = EvaluationFramework()\n",
    "\n",
    "async with AsyncSessionLocal() as db_session:\n",
    "    await evaluator.initialize_models(db_session, train_df=train_df)  # ‚Üê PASS TRAIN_DF!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d763b0d-d988-4028-9e08-7cec8eb29d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: BASELINE EVALUATION FUNCTION  ===\n",
    "def calculate_metrics(recommendations, ground_truth, k=10):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    \n",
    "    # Get top-k recommendations\n",
    "    rec_items = recommendations[:k]\n",
    "    \n",
    "    # Relevance-based metrics\n",
    "    hits = len(set(rec_items) & set(ground_truth))\n",
    "    \n",
    "    precision = hits / len(rec_items) if rec_items else 0.0\n",
    "    recall = hits / len(ground_truth) if ground_truth else 0.0\n",
    "    \n",
    "    # NDCG\n",
    "    dcg = sum(1/np.log2(i+2) for i, item in enumerate(rec_items) if item in ground_truth)\n",
    "    idcg = sum(1/np.log2(i+2) for i in range(min(k, len(ground_truth))))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    # Diversity (ILS - Intra-List Similarity)\n",
    "    if len(rec_items) > 1:\n",
    "        # Simplified diversity: unique categories in recommendations\n",
    "        diversity = len(set(rec_items)) / len(rec_items)\n",
    "    else:\n",
    "        diversity = 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'ndcg': ndcg,\n",
    "        'diversity': diversity,\n",
    "        'hits': hits\n",
    "    }\n",
    "\n",
    "async def evaluate_model(model, model_name, test_df, db_session, \n",
    "                        use_mmr=False, lambda_mmr=None, use_mab=False, \n",
    "                        context_service=None, mab_optimizer=None):\n",
    "    \"\"\"Evaluate single model with proper API handling\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Evaluating {model_name}...\")\n",
    "    \n",
    "    metrics_list = []\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    for user_id in tqdm(test_users[:100], desc=f\"  {model_name}\"):\n",
    "        try:\n",
    "            # Get ground truth\n",
    "            user_test = test_df[test_df['user_id'] == user_id]\n",
    "            ground_truth = user_test[user_test['rating'] >= 4.0]['item_id'].tolist()\n",
    "            \n",
    "            if not ground_truth:\n",
    "                continue\n",
    "            \n",
    "            # === HANDLE DIFFERENT MODEL APIS ===\n",
    "            rec_items = []\n",
    "            \n",
    "            if model_name in ['Content-Based', 'Collaborative Filtering']:\n",
    "                # OLD API: No db_session parameter\n",
    "                try:\n",
    "                    recommendations = await model.predict(\n",
    "                        user_id=user_id,\n",
    "                        num_recommendations=10\n",
    "                    )\n",
    "                    # Handle different return formats\n",
    "                    if isinstance(recommendations, list):\n",
    "                        if len(recommendations) > 0 and isinstance(recommendations[0], dict):\n",
    "                            rec_items = [r.get('destination_id') or r.get('item_id') for r in recommendations]\n",
    "                        else:\n",
    "                            rec_items = recommendations\n",
    "                    else:\n",
    "                        rec_items = []\n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ö†Ô∏è User {user_id} prediction failed: {str(e)[:50]}\")\n",
    "                    continue\n",
    "            \n",
    "            elif use_mab and context_service and mab_optimizer:\n",
    "                # MAB-MMR: context-aware dynamic Œª\n",
    "                context = await context_service.get_current_context()\n",
    "                recommendations, selected_arm = await model.predict(\n",
    "                    user_id=user_id,\n",
    "                    num_recommendations=10,\n",
    "                    db=db_session,\n",
    "                    lambda_mmr=None,\n",
    "                    mab_optimizer=mab_optimizer,\n",
    "                    context=context\n",
    "                )\n",
    "                rec_items = [r['destination_id'] for r in recommendations]\n",
    "                \n",
    "            elif use_mmr and lambda_mmr is not None:\n",
    "                # Static MMR\n",
    "                recommendations, _ = await model.predict(\n",
    "                    user_id=user_id,\n",
    "                    num_recommendations=10,\n",
    "                    db=db_session,\n",
    "                    lambda_mmr=lambda_mmr,\n",
    "                    mab_optimizer=None,\n",
    "                    context={}\n",
    "                )\n",
    "                rec_items = [r['destination_id'] for r in recommendations]\n",
    "                \n",
    "            else:\n",
    "                # Hybrid baseline (no reranking)\n",
    "                recommendations, _ = await model.predict(\n",
    "                    user_id=user_id,\n",
    "                    num_recommendations=10,\n",
    "                    db=db_session,\n",
    "                    lambda_mmr=0.7,  # Default lambda\n",
    "                    mab_optimizer=None,\n",
    "                    context=None\n",
    "                )\n",
    "                rec_items = [r['destination_id'] for r in recommendations]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if rec_items:\n",
    "                metrics = calculate_metrics(rec_items, ground_truth, k=10)\n",
    "                metrics_list.append(metrics)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è User {user_id} failed: {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    # Aggregate results\n",
    "    if not metrics_list:\n",
    "        print(f\"‚ö†Ô∏è {model_name}: No valid evaluations!\")\n",
    "        return None\n",
    "    \n",
    "    results = {\n",
    "        'precision@10': np.mean([m['precision'] for m in metrics_list]),\n",
    "        'recall@10': np.mean([m['recall'] for m in metrics_list]),\n",
    "        'ndcg@10': np.mean([m['ndcg'] for m in metrics_list]),\n",
    "        'diversity': np.mean([m['diversity'] for m in metrics_list]),\n",
    "        'n_users': len(metrics_list)\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} evaluated: {results['n_users']} users\")\n",
    "    print(f\"   Precision@10: {results['precision@10']:.4f}\")\n",
    "    print(f\"   Diversity: {results['diversity']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960ae09-4092-4ee7-a598-ff00f0974b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECK TRAINING STATUS ===\n",
    "print(\"üîç CHECKING MODEL TRAINING STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in evaluator.models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    \n",
    "    # Check if model has is_trained attribute\n",
    "    if hasattr(model, 'is_trained'):\n",
    "        print(f\"   is_trained flag: {model.is_trained}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è No is_trained attribute\")\n",
    "    \n",
    "    # Check internal models for Hybrid\n",
    "    if model_name.startswith('Hybrid'):\n",
    "        if hasattr(model, 'collaborative_recommender'):\n",
    "            cf_trained = getattr(model.collaborative_recommender, 'is_trained', False)\n",
    "            print(f\"   CF component trained: {cf_trained}\")\n",
    "        \n",
    "        if hasattr(model, 'content_recommender'):\n",
    "            cb_trained = getattr(model.content_recommender, 'is_trained', False)\n",
    "            print(f\"   CB component trained: {cb_trained}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9941cd7-eb19-41bb-b343-46132bb5c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: RUN ALL EVALUATIONS  ===\n",
    "async def run_comprehensive_evaluation():\n",
    "    \"\"\"Run evaluation on all models\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ COMPREHENSIVE BASELINE EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    async with AsyncSessionLocal() as db_session:\n",
    "        \n",
    "        # 1. Content-Based\n",
    "        results_cb = await evaluate_model(\n",
    "            evaluator.models['CB'], 'Content-Based', \n",
    "            test_df, db_session\n",
    "        )\n",
    "        all_results['Content-Based'] = results_cb\n",
    "        \n",
    "        # 2. Collaborative Filtering\n",
    "        results_cf = await evaluate_model(\n",
    "            evaluator.models['CF'], 'Collaborative Filtering',\n",
    "            test_df, db_session\n",
    "        )\n",
    "        all_results['Collaborative Filtering'] = results_cf\n",
    "        \n",
    "        # 3. Hybrid (no reranking)\n",
    "        results_hybrid = await evaluate_model(\n",
    "            evaluator.models['Hybrid'], 'Hybrid',\n",
    "            test_df, db_session\n",
    "        )\n",
    "        all_results['Hybrid'] = results_hybrid\n",
    "        \n",
    "        # 4. Hybrid + Static MMR (Œª=0.5)\n",
    "        results_mmr_static = await evaluate_model(\n",
    "            evaluator.models['Hybrid+MMR_Static'], 'Hybrid+MMR(Œª=0.5)',\n",
    "            test_df, db_session,\n",
    "            use_mmr=True, lambda_mmr=0.5\n",
    "        )\n",
    "        all_results['Hybrid+MMR_Static'] = results_mmr_static\n",
    "        \n",
    "        # 5. Hybrid + MAB-MMR (dynamic Œª) - THE PROPOSED METHOD\n",
    "        results_mab_mmr = await evaluate_model(\n",
    "            evaluator.models['Hybrid+MAB_MMR'], 'Hybrid+MAB-MMR',\n",
    "            test_df, db_session,\n",
    "            use_mmr=True, use_mab=True,\n",
    "            context_service=evaluator.context_service,\n",
    "            mab_optimizer=evaluator.mab_optimizer\n",
    "        )\n",
    "        all_results['Hybrid+MAB_MMR'] = results_mab_mmr\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Execute comprehensive evaluation\n",
    "evaluation_results = await run_comprehensive_evaluation()\n",
    "\n",
    "# Display results table\n",
    "results_df = pd.DataFrame(evaluation_results).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c2762-5ad7-458d-ae81-a449f7e1a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DIAGNOSTIC CELL ===\n",
    "print(\"üîç CHECKING EVALUATION STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if evaluation_results exists\n",
    "if 'evaluation_results' in globals():\n",
    "    print(\"‚úÖ evaluation_results variable exists\")\n",
    "    print(f\"   Models evaluated: {list(evaluation_results.keys())}\")\n",
    "    \n",
    "    for model, results in evaluation_results.items():\n",
    "        if results is None:\n",
    "            print(f\"   ‚ùå {model}: None (FAILED or NOT RUN)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ {model}: {results.get('n_users', 0)} users\")\n",
    "else:\n",
    "    print(\"‚ùå evaluation_results variable NOT FOUND!\")\n",
    "    print(\"   Did Cell 6 complete successfully?\")\n",
    "\n",
    "# Check if models are initialized\n",
    "if 'evaluator' in globals():\n",
    "    print(\"\\n‚úÖ evaluator exists\")\n",
    "    print(f\"   Models loaded: {list(evaluator.models.keys())}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå evaluator NOT FOUND!\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057cca3-b3e6-49a3-a0f7-3729a49b0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: STATISTICAL SIGNIFICANCE TESTS  ===\n",
    "def perform_statistical_tests(results_dict):\n",
    "    \"\"\"Perform paired t-tests for statistical significance\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä STATISTICAL SIGNIFICANCE TESTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Baseline comparison reference\n",
    "    baseline_models = ['Content-Based', 'Collaborative Filtering', 'Hybrid', 'Hybrid+MMR_Static']\n",
    "    proposed_model = 'Hybrid+MAB_MMR'\n",
    "    \n",
    "    significance_results = []\n",
    "    \n",
    "    for baseline in baseline_models:\n",
    "        if baseline not in results_dict or proposed_model not in results_dict:\n",
    "            continue\n",
    "        \n",
    "        # Get metric distributions (would need raw per-user metrics in real implementation)\n",
    "        # For now, demonstrate with summary statistics\n",
    "        \n",
    "        baseline_precision = results_dict[baseline]['precision@10']\n",
    "        proposed_precision = results_dict[proposed_model]['precision@10']\n",
    "        \n",
    "        baseline_diversity = results_dict[baseline]['diversity']\n",
    "        proposed_diversity = results_dict[proposed_model]['diversity']\n",
    "        \n",
    "        # Calculate improvement percentage\n",
    "        precision_improvement = ((proposed_precision - baseline_precision) / baseline_precision * 100) if baseline_precision > 0 else 0\n",
    "        diversity_improvement = ((proposed_diversity - baseline_diversity) / baseline_diversity * 100) if baseline_diversity > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüîç {proposed_model} vs {baseline}:\")\n",
    "        print(f\"   Precision improvement: {precision_improvement:+.2f}%\")\n",
    "        print(f\"   Diversity improvement: {diversity_improvement:+.2f}%\")\n",
    "        \n",
    "        significance_results.append({\n",
    "            'comparison': f'{proposed_model} vs {baseline}',\n",
    "            'precision_improvement': precision_improvement,\n",
    "            'diversity_improvement': diversity_improvement\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(significance_results)\n",
    "\n",
    "# Run statistical tests\n",
    "significance_df = perform_statistical_tests(evaluation_results)\n",
    "print(\"\\nüìä Statistical Significance Summary:\")\n",
    "print(significance_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f02d1-fe1d-437e-846e-0847a865924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 8: VISUALIZATIONS  ===\n",
    "def create_evaluation_visualizations(results_df, output_dir='evaluation_plots'):\n",
    "    \"\"\"Create publication-ready visualizations\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set publication quality\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    \n",
    "    # 1. Metrics Comparison Bar Chart\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    metrics = ['precision@10', 'recall@10', 'ndcg@10', 'diversity']\n",
    "    titles = ['Precision@10', 'Recall@10', 'NDCG@10', 'Diversity']\n",
    "    \n",
    "    for ax, metric, title in zip(axes.flat, metrics, titles):\n",
    "        results_df[metric].plot(kind='bar', ax=ax, color=sns.color_palette(\"husl\", len(results_df)))\n",
    "        ax.set_title(f'{title} Comparison', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.set_xticklabels(results_df.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Highlight proposed method\n",
    "        if 'MAB' in results_df.index[-1]:\n",
    "            ax.patches[-1].set_facecolor('red')\n",
    "            ax.patches[-1].set_alpha(0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/metrics_comparison.png', bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {output_dir}/metrics_comparison.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Precision vs Diversity Scatter\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['blue', 'blue', 'blue', 'green', 'red']\n",
    "    sizes = [100, 100, 100, 150, 200]\n",
    "    \n",
    "    for i, model in enumerate(results_df.index):\n",
    "        ax.scatter(results_df.loc[model, 'precision@10'], \n",
    "                  results_df.loc[model, 'diversity'],\n",
    "                  s=sizes[i], c=colors[i], alpha=0.6, edgecolors='black', linewidth=2)\n",
    "        ax.annotate(model, \n",
    "                   (results_df.loc[model, 'precision@10'], results_df.loc[model, 'diversity']),\n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Precision@10', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Diversity', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Precision vs Diversity Trade-off', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/precision_diversity_tradeoff.png', bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {output_dir}/precision_diversity_tradeoff.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Radar Chart\n",
    "    from math import pi\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    categories = ['Precision@10', 'Recall@10', 'NDCG@10', 'Diversity']\n",
    "    N = len(categories)\n",
    "    \n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontsize=11)\n",
    "    \n",
    "    # Plot each model\n",
    "    colors_radar = ['blue', 'orange', 'green', 'purple', 'red']\n",
    "    for i, model in enumerate(results_df.index):\n",
    "        values = [\n",
    "            results_df.loc[model, 'precision@10'],\n",
    "            results_df.loc[model, 'recall@10'],\n",
    "            results_df.loc[model, 'ndcg@10'],\n",
    "            results_df.loc[model, 'diversity']\n",
    "        ]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors_radar[i])\n",
    "        ax.fill(angles, values, alpha=0.15, color=colors_radar[i])\n",
    "    \n",
    "    ax.set_ylim(0, max(results_df[metrics].max()) * 1.1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "    ax.set_title('Multi-Metric Performance Comparison', size=14, fontweight='bold', pad=20)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/radar_chart.png', bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {output_dir}/radar_chart.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ All visualizations saved to {output_dir}/\")\n",
    "\n",
    "# Create visualizations\n",
    "create_evaluation_visualizations(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf54eb4-1e78-4acf-84a4-c60d819e17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 9: EXPORT RESULTS ===\n",
    "def export_final_results(evaluation_results, results_df, significance_df, output_prefix='final_evaluation'):\n",
    "    \"\"\"Export all results in multiple formats for thesis\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. JSON (detailed results)\n",
    "    json_output = {\n",
    "        'timestamp': timestamp,\n",
    "        'dataset': {\n",
    "            'total_ratings': len(ratings_df),\n",
    "            'total_users': ratings_df['user_id'].nunique(),\n",
    "            'total_items': ratings_df['item_id'].nunique(),\n",
    "            'train_size': len(train_df),\n",
    "            'test_size': len(test_df)\n",
    "        },\n",
    "        'models_evaluated': list(evaluation_results.keys()),\n",
    "        'results': evaluation_results,\n",
    "        'statistical_significance': significance_df.to_dict('records')\n",
    "    }\n",
    "    \n",
    "    json_file = f'{output_prefix}_{timestamp}.json'\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(json_output, f, indent=2)\n",
    "    print(f\"‚úÖ Saved JSON: {json_file}\")\n",
    "    \n",
    "    # 2. CSV (results table)\n",
    "    csv_file = f'{output_prefix}_{timestamp}.csv'\n",
    "    results_df.to_csv(csv_file)\n",
    "    print(f\"‚úÖ Saved CSV: {csv_file}\")\n",
    "    \n",
    "    # 3. Pickle (full evaluation object for reproducibility)\n",
    "    pkl_file = f'{output_prefix}_{timestamp}.pkl'\n",
    "    with open(pkl_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'results': evaluation_results,\n",
    "            'results_df': results_df,\n",
    "            'significance_df': significance_df,\n",
    "            'user_activity_df': user_activity_df,\n",
    "            'timestamp': timestamp\n",
    "        }, f)\n",
    "    print(f\"‚úÖ Saved Pickle: {pkl_file}\")\n",
    "    \n",
    "    # 4. LaTeX table (for thesis)\n",
    "    latex_file = f'{output_prefix}_{timestamp}.tex'\n",
    "    with open(latex_file, 'w') as f:\n",
    "        f.write(\"% Evaluation Results Table for Thesis\\n\")\n",
    "        f.write(\"\\\\begin{table}[htbp]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{Comparative Evaluation Results}\\n\")\n",
    "        f.write(\"\\\\label{tab:evaluation_results}\\n\")\n",
    "        f.write(results_df.to_latex(float_format=\"%.4f\"))\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "    print(f\"‚úÖ Saved LaTeX: {latex_file}\")\n",
    "    \n",
    "    print(f\"\\nüéâ All results exported successfully!\")\n",
    "    return json_file, csv_file, pkl_file, latex_file\n",
    "\n",
    "# Export all results\n",
    "json_file, csv_file, pkl_file, latex_file = export_final_results(\n",
    "    evaluation_results, results_df, significance_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966b0e5-f775-4844-a606-66bb2765716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 10: SUMMARY REPORT ===\n",
    "def print_final_summary(results_df):\n",
    "    \"\"\"Print comprehensive summary for thesis documentation\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéì FINAL EVALUATION SUMMARY\")\n",
    "    print(\"Academic Paper: Proving MAB-MMR Superiority\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best performing model\n",
    "    best_precision = results_df['precision@10'].idxmax()\n",
    "    best_diversity = results_df['diversity'].idxmax()\n",
    "    best_ndcg = results_df['ndcg@10'].idxmax()\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
    "    print(f\"   Precision@10: {best_precision} ({results_df.loc[best_precision, 'precision@10']:.4f})\")\n",
    "    print(f\"   Diversity: {best_diversity} ({results_df.loc[best_diversity, 'diversity']:.4f})\")\n",
    "    print(f\"   NDCG@10: {best_ndcg} ({results_df.loc[best_ndcg, 'ndcg@10']:.4f})\")\n",
    "    \n",
    "    # MAB-MMR performance\n",
    "    if 'Hybrid+MAB_MMR' in results_df.index:\n",
    "        mab_results = results_df.loc['Hybrid+MAB_MMR']\n",
    "        print(f\"\\nüéØ PROPOSED METHOD (Hybrid+MAB-MMR) PERFORMANCE:\")\n",
    "        print(f\"   Precision@10: {mab_results['precision@10']:.4f}\")\n",
    "        print(f\"   Recall@10: {mab_results['recall@10']:.4f}\")\n",
    "        print(f\"   NDCG@10: {mab_results['ndcg@10']:.4f}\")\n",
    "        print(f\"   Diversity: {mab_results['diversity']:.4f}\")\n",
    "        print(f\"   Users evaluated: {int(mab_results['n_users'])}\")\n",
    "        \n",
    "        # Compare to best baseline\n",
    "        best_baseline_precision = results_df.drop('Hybrid+MAB_MMR')['precision@10'].max()\n",
    "        improvement = ((mab_results['precision@10'] - best_baseline_precision) / best_baseline_precision * 100)\n",
    "        \n",
    "        print(f\"\\nüìà IMPROVEMENT OVER BEST BASELINE:\")\n",
    "        print(f\"   Precision improvement: {improvement:+.2f}%\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"üìÅ Results saved in multiple formats (JSON, CSV, Pickle, LaTeX)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Print final summary\n",
    "print_final_summary(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
