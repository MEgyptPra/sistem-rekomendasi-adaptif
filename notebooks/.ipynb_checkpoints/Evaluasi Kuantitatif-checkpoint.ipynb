{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b187160-9c98-45ae-8016-b2e7ee11da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.0.34)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: asyncpg in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.30.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.14.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from asyncpg) (4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy psycopg2-binary nest_asyncio asyncpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "719b8649-e9f7-4f76-9544-b621da6250cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine siap.\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL 1: SETUP DAN IMPORT LIBRARIES =====\n",
    "import nest_asyncio, asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Database configuration\n",
    "DATABASE_URL = \"postgresql+asyncpg://user:rekompari@localhost:5432/pariwisata\"\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=False, future=True)\n",
    "AsyncSessionLocal = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)\n",
    "\n",
    "print(\"Engine siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "654c7222-2579-4d96-a689-6ed98a569758",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../pariwisata-recommender/backend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollaborative_recommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CollaborativeRecommender\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontent_based_recommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContentBasedRecommender\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhybrid_recommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HybridRecommender\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "# ===== CELL 2: IMPORT MODULES =====\n",
    "import sys\n",
    "sys.path.append('../pariwisata-recommender/backend')\n",
    "\n",
    "from app.services.collaborative_recommender import CollaborativeRecommender\n",
    "from app.services.content_based_recommender import ContentBasedRecommender\n",
    "from app.services.hybrid_recommender import HybridRecommender\n",
    "from app.services.mab_optimizer import MABOptimizer \n",
    "from app.services.ml_service import MLService \n",
    "from app.services.real_time_data import RealTimeContextService as ContextScorer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Semua modul berhasil di-import.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "496acc22-4a28-4d82-8213-b0dbc5e695b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 3: TRAINING FUNCTION UNTUK CF =====\n",
    "async def collaborative_train_from_df(model, ratings_df):\n",
    "    if len(ratings_df) < 10:\n",
    "        raise ValueError(\"Not enough ratings (min 10).\")\n",
    "    pivot = ratings_df.pivot_table(index='user_id', columns='destination_id', values='rating', aggfunc='mean').fillna(0)\n",
    "    model.user_item_matrix = pivot\n",
    "    users = pivot.index.tolist()\n",
    "    items = pivot.columns.tolist()\n",
    "    model.user_encoder = {u:i for i,u in enumerate(users)}\n",
    "    model.item_encoder = {it:j for j,it in enumerate(items)}\n",
    "    model.user_decoder = {i:u for u,i in model.user_encoder.items()}\n",
    "    model.item_decoder = {j:it for it,j in model.item_encoder.items()}\n",
    "    M = pivot.values\n",
    "    model.user_factors = model.nmf_model.fit_transform(M)\n",
    "    model.item_factors = model.nmf_model.components_.T\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    model.user_similarities = cosine_similarity(model.user_factors)\n",
    "    model.is_trained = True\n",
    "    return {\"status\":\"success\",\"users_count\":len(users),\"items_count\":len(items),\"ratings_count\":len(ratings_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2af0603-b3e0-4be3-a656-cd9d28a120d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===== CELL 4: LOAD DAN SPLIT DATA =====\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rating\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_ratings_df\u001b[39m():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncSessionLocal() \u001b[38;5;28;01mas\u001b[39;00m db:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "# ===== CELL 4: LOAD DAN SPLIT DATA =====\n",
    "from sqlalchemy import select\n",
    "from app.models.rating import Rating\n",
    "\n",
    "async def load_ratings_df():\n",
    "    async with AsyncSessionLocal() as db:\n",
    "        res = await db.execute(select(Rating))\n",
    "        rows = res.scalars().all()\n",
    "    data = [{'user_id': r.user_id, 'destination_id': r.destination_id, 'rating': float(r.rating)} for r in rows]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "ratings_df = await load_ratings_df()\n",
    "print(\"Total ratings:\", len(ratings_df))\n",
    "\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cb2004-c848-44f7-8f0c-850c00d5ac70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CollaborativeRecommender' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===== CELL 5: TRAIN CF MODEL =====\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m collab_model \u001b[38;5;241m=\u001b[39m \u001b[43mCollaborativeRecommender\u001b[49m()\n\u001b[0;32m      3\u001b[0m train_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m collaborative_train_from_df(collab_model, train_df)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCF Model trained:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_info)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CollaborativeRecommender' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== CELL 5: TRAIN CF MODEL =====\n",
    "collab_model = CollaborativeRecommender()\n",
    "train_info = await collaborative_train_from_df(collab_model, train_df)\n",
    "print(\"CF Model trained:\", train_info)\n",
    "\n",
    "async def cf_predict(user_id, k=10):\n",
    "    async with AsyncSessionLocal() as db:\n",
    "        recs = await collab_model.predict(user_id=user_id, num_recommendations=k, db=db)\n",
    "    return recs\n",
    "\n",
    "# Test CF model\n",
    "test_user = train_df.user_id.iloc[0]\n",
    "sample = await cf_predict(test_user, k=5)\n",
    "print(\"Sample CF recommendations:\", [r['destination_id'] for r in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971a3116-3375-47c7-94dc-5e4de2d49c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 6: EVALUATION METRICS =====\n",
    "def precision_at_k(recs_ids, ground_truth_ids, k=10):\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    return len(set(recs_ids[:k]) & set(ground_truth_ids)) / k\n",
    "\n",
    "def recall_at_k(recs_ids, ground_truth_ids, k=10):\n",
    "    if len(ground_truth_ids) == 0:\n",
    "        return 0.0\n",
    "    return len(set(recs_ids[:k]) & set(ground_truth_ids)) / len(ground_truth_ids)\n",
    "\n",
    "def ndcg_at_k(recs_ids, ground_truth_ids, k=10):\n",
    "    dcg = 0.0\n",
    "    for i, did in enumerate(recs_ids[:k]):\n",
    "        if did in ground_truth_ids:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    ideal_hits = min(len(ground_truth_ids), k)\n",
    "    if ideal_hits == 0:\n",
    "        return 0.0\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c9bf27-75d2-4fe3-b4bc-939d07116654",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===== CELL 7: PREPARE TEST DATA =====\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Ground truth: item yang user rating di test set\u001b[39;00m\n\u001b[0;32m      3\u001b[0m test_truth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtest_df\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdestination_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m eligible_users \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     uid \u001b[38;5;28;01mfor\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m test_truth\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m collab_model\u001b[38;5;241m.\u001b[39muser_encoder\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEligible users for evaluation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eligible_users)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== CELL 7: PREPARE TEST DATA =====\n",
    "# Ground truth: item yang user rating di test set\n",
    "test_truth = (\n",
    "    test_df\n",
    "    .groupby('user_id')['destination_id']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "eligible_users = [\n",
    "    uid for uid in test_truth.keys()\n",
    "    if uid in collab_model.user_encoder\n",
    "]\n",
    "\n",
    "print(f\"Eligible users for evaluation: {len(eligible_users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3becfb2-80aa-4f07-adba-59e95f2b03ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All 5 models implemented!\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL 8: IMPLEMENTASI 5 MODEL =====\n",
    "\n",
    "# 1. CONTENT-BASED MODEL\n",
    "class ProperContentBasedRecommender:\n",
    "    def __init__(self):\n",
    "        self.destinations_data = None\n",
    "        self.category_popularity = None\n",
    "        \n",
    "    async def train(self):\n",
    "        \"\"\"Train dengan data yang sudah ada di train_df\"\"\"\n",
    "        # Get unique destinations from ratings\n",
    "        unique_destinations = train_df['destination_id'].unique()\n",
    "        \n",
    "        # Simulate destination categories (karena tidak ada akses ke model DB)\n",
    "        categories = ['Wisata Alam', 'Wisata Sejarah', 'Wisata Kuliner', 'Wisata Buatan', 'Wisata Keluarga']\n",
    "        \n",
    "        # Create destination data\n",
    "        self.destinations_data = {}\n",
    "        for dest_id in unique_destinations:\n",
    "            # Assign category based on destination_id (deterministic)\n",
    "            category = categories[dest_id % len(categories)]\n",
    "            self.destinations_data[dest_id] = {\n",
    "                'category': category,\n",
    "                'popularity': len(train_df[train_df['destination_id'] == dest_id])\n",
    "            }\n",
    "        \n",
    "        # Calculate category popularity\n",
    "        self.category_popularity = {}\n",
    "        for category in categories:\n",
    "            cat_dest_ids = [did for did, data in self.destinations_data.items() if data['category'] == category]\n",
    "            self.category_popularity[category] = train_df[train_df['destination_id'].isin(cat_dest_ids)]['destination_id'].value_counts()\n",
    "    \n",
    "    async def predict(self, user_id, num_recommendations=10):\n",
    "        \"\"\"Generate content-based recommendations\"\"\"\n",
    "        user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            # Cold start: recommend popular items from each category\n",
    "            recommendations = []\n",
    "            for category, pop_series in self.category_popularity.items():\n",
    "                recommendations.extend(pop_series.head(2).index.tolist())\n",
    "            \n",
    "            return [{\n",
    "                'destination_id': dest_id,\n",
    "                'score': 1.0 - (i * 0.1),\n",
    "                'name': f'Destination_{dest_id}',\n",
    "                'explanation': 'Content-based (cold start)',\n",
    "                'algorithm': 'content_based'\n",
    "            } for i, dest_id in enumerate(recommendations[:num_recommendations])]\n",
    "        \n",
    "        # Analyze user's category preferences\n",
    "        user_categories = []\n",
    "        for _, rating in user_ratings.iterrows():\n",
    "            dest_id = rating['destination_id']\n",
    "            if dest_id in self.destinations_data:\n",
    "                user_categories.append(self.destinations_data[dest_id]['category'])\n",
    "        \n",
    "        if not user_categories:\n",
    "            return []\n",
    "        \n",
    "        # Get preferred categories\n",
    "        category_counts = Counter(user_categories)\n",
    "        preferred_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Generate recommendations based on preferred categories\n",
    "        recommendations = []\n",
    "        rated_items = set(user_ratings['destination_id'].tolist())\n",
    "        \n",
    "        for category, _ in preferred_categories:\n",
    "            if category in self.category_popularity:\n",
    "                for dest_id in self.category_popularity[category].index:\n",
    "                    if dest_id not in rated_items and dest_id not in recommendations:\n",
    "                        recommendations.append(dest_id)\n",
    "                        if len(recommendations) >= num_recommendations:\n",
    "                            break\n",
    "            if len(recommendations) >= num_recommendations:\n",
    "                break\n",
    "        \n",
    "        return [{\n",
    "            'destination_id': dest_id,\n",
    "            'score': 1.0 - (i * 0.05),\n",
    "            'name': f'Destination_{dest_id}',\n",
    "            'explanation': 'Content-based recommendation',\n",
    "            'algorithm': 'content_based'\n",
    "        } for i, dest_id in enumerate(recommendations[:num_recommendations])]\n",
    "\n",
    "# 2. CONTEXT-AWARE COMPONENT\n",
    "class ContextAwareComponent:\n",
    "    def __init__(self):\n",
    "        self.context_weights = {\n",
    "            'morning': {'Wisata Alam': 1.2, 'Wisata Sejarah': 1.0, 'Wisata Kuliner': 0.8},\n",
    "            'afternoon': {'Wisata Kuliner': 1.2, 'Wisata Buatan': 1.1, 'Wisata Keluarga': 1.0},\n",
    "            'evening': {'Wisata Kuliner': 1.3, 'Wisata Buatan': 1.0, 'Wisata Keluarga': 0.9}\n",
    "        }\n",
    "    \n",
    "    def get_context_boost(self, destination_id, context=None):\n",
    "        \"\"\"Get context-based score boost\"\"\"\n",
    "        if context is None:\n",
    "            context = {'time_of_day': 'afternoon'}  # Default\n",
    "        \n",
    "        # Simulate context influence\n",
    "        time_of_day = context.get('time_of_day', 'afternoon')\n",
    "        \n",
    "        # Get destination category (simplified)\n",
    "        categories = ['Wisata Alam', 'Wisata Sejarah', 'Wisata Kuliner', 'Wisata Buatan', 'Wisata Keluarga']\n",
    "        category = categories[destination_id % len(categories)]\n",
    "        \n",
    "        weights = self.context_weights.get(time_of_day, {})\n",
    "        return weights.get(category, 1.0)\n",
    "\n",
    "# 3. HYBRID MODEL\n",
    "class ProperHybridRecommender:\n",
    "    def __init__(self, cf_model, cb_model, context_component, cf_weight=0.5, cb_weight=0.3, context_weight=0.2):\n",
    "        self.cf_model = cf_model\n",
    "        self.cb_model = cb_model\n",
    "        self.context_component = context_component\n",
    "        self.cf_weight = cf_weight\n",
    "        self.cb_weight = cb_weight\n",
    "        self.context_weight = context_weight\n",
    "    \n",
    "    async def predict(self, user_id, num_recommendations=10, context=None):\n",
    "        \"\"\"Generate hybrid recommendations\"\"\"\n",
    "        try:\n",
    "            # Get CF recommendations\n",
    "            cf_recs = await self.cf_model.predict(user_id=user_id, num_recommendations=num_recommendations*2, db=AsyncSessionLocal())\n",
    "            cf_scores = {r['destination_id']: r['score'] * self.cf_weight for r in cf_recs}\n",
    "            \n",
    "            # Get CB recommendations\n",
    "            cb_recs = await self.cb_model.predict(user_id=user_id, num_recommendations=num_recommendations*2)\n",
    "            cb_scores = {r['destination_id']: r['score'] * self.cb_weight for r in cb_recs}\n",
    "            \n",
    "            # Combine scores with context\n",
    "            combined_scores = {}\n",
    "            all_items = set(cf_scores.keys()) | set(cb_scores.keys())\n",
    "            \n",
    "            for item_id in all_items:\n",
    "                base_score = cf_scores.get(item_id, 0) + cb_scores.get(item_id, 0)\n",
    "                context_boost = self.context_component.get_context_boost(item_id, context) * self.context_weight\n",
    "                combined_scores[item_id] = base_score + context_boost\n",
    "            \n",
    "            # Sort and return top k\n",
    "            sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return [{\n",
    "                'destination_id': item_id,\n",
    "                'score': score,\n",
    "                'name': f'Destination_{item_id}',\n",
    "                'explanation': 'Hybrid (CF+CB+Context)',\n",
    "                'algorithm': 'hybrid'\n",
    "            } for item_id, score in sorted_items[:num_recommendations]]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Hybrid error: {e}\")\n",
    "            # Fallback to CF\n",
    "            return await self.cf_model.predict(user_id=user_id, num_recommendations=num_recommendations, db=AsyncSessionLocal())\n",
    "\n",
    "# 4. MMR RERANKER\n",
    "class MMRReranker:\n",
    "    def __init__(self, lambda_param=0.5):\n",
    "        self.lambda_param = lambda_param\n",
    "    \n",
    "    def calculate_similarity(self, item1, item2):\n",
    "        \"\"\"Calculate similarity between two items (simplified)\"\"\"\n",
    "        # Based on destination categories\n",
    "        categories = ['Wisata Alam', 'Wisata Sejarah', 'Wisata Kuliner', 'Wisata Buatan', 'Wisata Keluarga']\n",
    "        cat1 = categories[item1 % len(categories)]\n",
    "        cat2 = categories[item2 % len(categories)]\n",
    "        return 1.0 if cat1 == cat2 else 0.3  # High similarity if same category\n",
    "    \n",
    "    async def rerank(self, recommendations, user_id):\n",
    "        \"\"\"Apply MMR re-ranking\"\"\"\n",
    "        if len(recommendations) <= 1:\n",
    "            return recommendations\n",
    "        \n",
    "        selected = []\n",
    "        candidates = recommendations.copy()\n",
    "        \n",
    "        # Select first item (highest relevance)\n",
    "        selected.append(candidates.pop(0))\n",
    "        \n",
    "        # MMR selection process\n",
    "        while candidates and len(selected) < len(recommendations):\n",
    "            best_score = -float('inf')\n",
    "            best_idx = 0\n",
    "            \n",
    "            for i, candidate in enumerate(candidates):\n",
    "                relevance = candidate['score']\n",
    "                \n",
    "                # Calculate max similarity to selected items\n",
    "                max_similarity = 0\n",
    "                for sel_item in selected:\n",
    "                    similarity = self.calculate_similarity(\n",
    "                        candidate['destination_id'], \n",
    "                        sel_item['destination_id']\n",
    "                    )\n",
    "                    max_similarity = max(max_similarity, similarity)\n",
    "                \n",
    "                # MMR score\n",
    "                mmr_score = self.lambda_param * relevance - (1 - self.lambda_param) * max_similarity\n",
    "                \n",
    "                if mmr_score > best_score:\n",
    "                    best_score = mmr_score\n",
    "                    best_idx = i\n",
    "            \n",
    "            selected.append(candidates.pop(best_idx))\n",
    "        \n",
    "        return selected\n",
    "\n",
    "# 5. MAB OPTIMIZER\n",
    "class SimpleMAB:\n",
    "    def __init__(self, n_arms=3):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = [0] * n_arms\n",
    "        self.values = [0.0] * n_arms\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def select_arm(self, context=None):\n",
    "        \"\"\"UCB1 arm selection\"\"\"\n",
    "        if self.total_count < self.n_arms:\n",
    "            return self.total_count\n",
    "        \n",
    "        ucb_values = []\n",
    "        for i in range(self.n_arms):\n",
    "            if self.counts[i] == 0:\n",
    "                return i\n",
    "            \n",
    "            confidence = (2 * np.log(self.total_count) / self.counts[i]) ** 0.5\n",
    "            ucb_values.append(self.values[i] + confidence)\n",
    "        \n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Update arm with reward\"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        self.total_count += 1\n",
    "        \n",
    "        n = self.counts[arm]\n",
    "        old_value = self.values[arm]\n",
    "        self.values[arm] = ((n - 1) / n) * old_value + (1 / n) * reward\n",
    "    \n",
    "    def get_lambda_for_mmr(self, arm):\n",
    "        \"\"\"Get adaptive lambda for MMR\"\"\"\n",
    "        lambdas = [0.8, 0.3, 0.5]  # relevance-focused, diversity-focused, balanced\n",
    "        return lambdas[arm]\n",
    "\n",
    "print(\"‚úÖ All 5 models implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6296c0c-9299-46ae-8cf8-9d2746bc02db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating evaluation with 5 proper models...\n",
      "üîß Initializing models...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'collab_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_df\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Execute evaluation\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m evaluation_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_final_evaluation()\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36mcreate_final_evaluation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize models\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîß Initializing models...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m cf_model \u001b[38;5;241m=\u001b[39m collab_model  \u001b[38;5;66;03m# Already trained\u001b[39;00m\n\u001b[0;32m      9\u001b[0m cb_model \u001b[38;5;241m=\u001b[39m ProperContentBasedRecommender()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m cb_model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collab_model' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== CELL 9: CREATE EVALUATION DATAFRAME =====\n",
    "async def create_final_evaluation():\n",
    "    print(\"üöÄ Creating evaluation with 5 proper models...\")\n",
    "    \n",
    "    # Initialize models\n",
    "    print(\"üîß Initializing models...\")\n",
    "    cf_model = collab_model  # Already trained\n",
    "    \n",
    "    cb_model = ProperContentBasedRecommender()\n",
    "    await cb_model.train()\n",
    "    \n",
    "    context_component = ContextAwareComponent()\n",
    "    \n",
    "    hybrid_model = ProperHybridRecommender(cf_model, cb_model, context_component)\n",
    "    \n",
    "    mmr_static = MMRReranker(lambda_param=0.5)\n",
    "    mmr_adaptive = MMRReranker(lambda_param=0.5)  # Will be adjusted by MAB\n",
    "    \n",
    "    mab_optimizer = SimpleMAB(n_arms=3)\n",
    "    \n",
    "    # Prepare evaluation\n",
    "    evaluation_users = eligible_users[:200]  # Limit for faster execution\n",
    "    all_recommendations = {\n",
    "        'cf': [], 'cb': [], 'hybrid': [], 'hybrid_mmr_statis': [], 'hybrid_mab_mmr': []\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Generating recommendations for {len(evaluation_users)} users...\")\n",
    "    \n",
    "    for i, user_id in enumerate(evaluation_users):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Progress: {i+1}/{len(evaluation_users)}\")\n",
    "        \n",
    "        # Generate context for this user\n",
    "        contexts = [\n",
    "            {'time_of_day': 'morning'},\n",
    "            {'time_of_day': 'afternoon'},\n",
    "            {'time_of_day': 'evening'}\n",
    "        ]\n",
    "        user_context = contexts[user_id % len(contexts)]\n",
    "        \n",
    "        try:\n",
    "            # 1. CF Model\n",
    "            cf_recs = await cf_model.predict(user_id=user_id, num_recommendations=10, db=AsyncSessionLocal())\n",
    "            cf_ids = [r['destination_id'] for r in cf_recs]\n",
    "            all_recommendations['cf'].append(cf_ids)\n",
    "            \n",
    "            # 2. CB Model\n",
    "            cb_recs = await cb_model.predict(user_id=user_id, num_recommendations=10)\n",
    "            cb_ids = [r['destination_id'] for r in cb_recs]\n",
    "            all_recommendations['cb'].append(cb_ids)\n",
    "            \n",
    "            # 3. Hybrid Model\n",
    "            hybrid_recs = await hybrid_model.predict(user_id=user_id, num_recommendations=10, context=user_context)\n",
    "            hybrid_ids = [r['destination_id'] for r in hybrid_recs]\n",
    "            all_recommendations['hybrid'].append(hybrid_ids)\n",
    "            \n",
    "            # 4. Hybrid + MMR Static\n",
    "            mmr_static_recs = await mmr_static.rerank(hybrid_recs.copy(), user_id)\n",
    "            mmr_static_ids = [r['destination_id'] for r in mmr_static_recs]\n",
    "            all_recommendations['hybrid_mmr_statis'].append(mmr_static_ids)\n",
    "            \n",
    "            # 5. Hybrid + MAB-MMR (Proposed Model)\n",
    "            # MAB arm selection\n",
    "            selected_arm = mab_optimizer.select_arm(user_context)\n",
    "            adaptive_lambda = mab_optimizer.get_lambda_for_mmr(selected_arm)\n",
    "            mmr_adaptive.lambda_param = adaptive_lambda\n",
    "            \n",
    "            mab_mmr_recs = await mmr_adaptive.rerank(hybrid_recs.copy(), user_id)\n",
    "            mab_mmr_ids = [r['destination_id'] for r in mab_mmr_recs]\n",
    "            all_recommendations['hybrid_mab_mmr'].append(mab_mmr_ids)\n",
    "            \n",
    "            # Simulate user feedback for MAB\n",
    "            simulated_reward = np.random.beta(2, 2)  # Realistic reward\n",
    "            mab_optimizer.update(selected_arm, simulated_reward)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for user {user_id}: {e}\")\n",
    "            # Add empty lists as fallback\n",
    "            for model in all_recommendations.keys():\n",
    "                if len(all_recommendations[model]) <= i:\n",
    "                    all_recommendations[model].append([])\n",
    "    \n",
    "    # Create evaluation DataFrame\n",
    "    evaluation_df = pd.DataFrame({'user_id': evaluation_users})\n",
    "    for model_name, recs in all_recommendations.items():\n",
    "        evaluation_df[f'recommendations_{model_name}'] = recs\n",
    "    \n",
    "    print(f\"‚úÖ evaluation_df created with shape: {evaluation_df.shape}\")\n",
    "    return evaluation_df\n",
    "\n",
    "# Execute evaluation\n",
    "evaluation_df = await create_final_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc3b6e5-5a8c-4d6c-8a96-fb712569b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFIKASI PERBEDAAN MODEL ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== VERIFIKASI PERBEDAAN MODEL ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_df\u001b[49m\u001b[38;5;241m.\u001b[39miloc[sample_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample User ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid_mmr_statis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid_mab_mmr\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluation_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== CELL 10: VERIFIKASI PERBEDAAN MODEL =====\n",
    "print(\"=== VERIFIKASI PERBEDAAN MODEL ===\")\n",
    "sample_idx = 0\n",
    "user_id = evaluation_df.iloc[sample_idx]['user_id']\n",
    "print(f\"Sample User ID: {user_id}\")\n",
    "\n",
    "for model_name in ['cf', 'cb', 'hybrid', 'hybrid_mmr_statis', 'hybrid_mab_mmr']:\n",
    "    recs = evaluation_df.iloc[sample_idx][f'recommendations_{model_name}']\n",
    "    print(f\"{model_name:20}: {recs[:5] if len(recs) >= 5 else recs}\")\n",
    "\n",
    "# Check differences\n",
    "print(\"\\n=== PERBANDINGAN ANTAR MODEL ===\")\n",
    "cf_recs = set(evaluation_df.iloc[0]['recommendations_cf'])\n",
    "for model_name in ['cb', 'hybrid', 'hybrid_mmr_statis', 'hybrid_mab_mmr']:\n",
    "    model_recs = set(evaluation_df.iloc[0][f'recommendations_{model_name}'])\n",
    "    overlap = len(cf_recs.intersection(model_recs))\n",
    "    print(f\"CF vs {model_name}: {overlap}/10 overlap ({'IDENTIK' if overlap == 10 else 'BERBEDA'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838bb82-9300-4f0f-a169-f6daa6a8ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 11: ANALISIS DISTRIBUSI KUANTIL =====\n",
    "print(\"\\n=== ANALISIS DISTRIBUSI KUANTIL REKOMENDASI ===\")\n",
    "\n",
    "def analyze_quantile_distribution(evaluation_df, model_name, total_items=231):\n",
    "    \"\"\"Analisis distribusi kuantil per model\"\"\"\n",
    "    all_recs = []\n",
    "    for recs in evaluation_df[f'recommendations_{model_name}']:\n",
    "        if isinstance(recs, list) and len(recs) > 0:\n",
    "            all_recs.extend(recs)\n",
    "    \n",
    "    if len(all_recs) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Hitung frekuensi item\n",
    "    item_counts = pd.Series(all_recs).value_counts().sort_values(ascending=False)\n",
    "    total_recs = len(all_recs)\n",
    "    unique_items = len(item_counts)\n",
    "    \n",
    "    # Hitung kuantil\n",
    "    top_10_pct_items = int(0.1 * total_items)  # 23 items\n",
    "    bottom_50_pct_items = int(0.5 * total_items)  # 116 items\n",
    "    \n",
    "    # Top 10% items share\n",
    "    top_10_share = item_counts.head(top_10_pct_items).sum() / total_recs * 100\n",
    "    \n",
    "    # Bottom 50% items share (items with lowest frequency)\n",
    "    all_items_ranked = item_counts.sort_values(ascending=True)\n",
    "    bottom_50_items_count = min(bottom_50_pct_items, len(all_items_ranked))\n",
    "    bottom_50_share = all_items_ranked.head(bottom_50_items_count).sum() / total_recs * 100\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'total_recs': total_recs,\n",
    "        'unique_items': unique_items,\n",
    "        'top_10_share': top_10_share,\n",
    "        'bottom_50_share': bottom_50_share,\n",
    "        'item_counts': item_counts\n",
    "    }\n",
    "\n",
    "# Analisis untuk semua model\n",
    "results = {}\n",
    "model_names = ['cf', 'cb', 'hybrid', 'hybrid_mmr_statis', 'hybrid_mab_mmr']\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nüîç Analyzing {model_name}...\")\n",
    "    result = analyze_quantile_distribution(evaluation_df, model_name)\n",
    "    \n",
    "    if result:\n",
    "        results[model_name] = result\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Total recommendations: {result['total_recs']}\")\n",
    "        print(f\"  Unique items recommended: {result['unique_items']}\")\n",
    "        print(f\"  Top 10% items (23 items): {result['top_10_share']:.1f}%\")\n",
    "        print(f\"  Bottom 50% items: {result['bottom_50_share']:.1f}%\")\n",
    "        \n",
    "        # Top 5 most recommended\n",
    "        top_5 = result['item_counts'].head(5)\n",
    "        print(f\"  Top 5 most recommended: {dict(top_5)}\")\n",
    "\n",
    "# Verifikasi perbedaan distribusi\n",
    "print(f\"\\n=== PERBANDINGAN DISTRIBUSI ANTAR MODEL ===\")\n",
    "if len(results) >= 2:\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            \n",
    "            if model1 in results and model2 in results:\n",
    "                diff_top10 = abs(results[model1]['top_10_share'] - results[model2]['top_10_share'])\n",
    "                diff_bottom50 = abs(results[model1]['bottom_50_share'] - results[model2]['bottom_50_share'])\n",
    "                \n",
    "                print(f\"{model1} vs {model2}:\")\n",
    "                print(f\"  Top 10% difference: {diff_top10:.2f}%\")\n",
    "                print(f\"  Bottom 50% difference: {diff_bottom50:.2f}%\")\n",
    "                print(f\"  Status: {'BERBEDA' if diff_top10 > 1.0 or diff_bottom50 > 1.0 else 'MIRIP'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986faa5-aadc-4f10-8f19-de32c41579a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 12: EVALUASI PERFORMA MODEL =====\n",
    "async def evaluate_all_models():\n",
    "    print(\"\\n=== EVALUASI PERFORMA SEMUA MODEL ===\")\n",
    "    \n",
    "    K = 10\n",
    "    model_performance = {}\n",
    "    \n",
    "    # Subset users for evaluation\n",
    "    eval_users = eligible_users[:100]  # Smaller subset for faster execution\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nüìä Evaluating {model_name}...\")\n",
    "        \n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        \n",
    "        for user_id in eval_users:\n",
    "            gt = test_truth.get(user_id, [])\n",
    "            if not gt:\n",
    "                continue\n",
    "            \n",
    "            # Get recommendations for this user\n",
    "            user_idx = evaluation_df[evaluation_df['user_id'] == user_id].index\n",
    "            if len(user_idx) == 0:\n",
    "                continue\n",
    "                \n",
    "            recs = evaluation_df.loc[user_idx[0], f'recommendations_{model_name}']\n",
    "            \n",
    "            if isinstance(recs, list) and len(recs) > 0:\n",
    "                p = precision_at_k(recs, gt, K)\n",
    "                r = recall_at_k(recs, gt, K)  \n",
    "                n = ndcg_at_k(recs, gt, K)\n",
    "                \n",
    "                precisions.append(p)\n",
    "                recalls.append(r)\n",
    "                ndcgs.append(n)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        if precisions:\n",
    "            model_performance[model_name] = {\n",
    "                'precision@10': np.mean(precisions),\n",
    "                'recall@10': np.mean(recalls),\n",
    "                'ndcg@10': np.mean(ndcgs),\n",
    "                'users_evaluated': len(precisions)\n",
    "            }\n",
    "        \n",
    "        print(f\"  Precision@10: {np.mean(precisions):.4f}\")\n",
    "        print(f\"  Recall@10: {np.mean(recalls):.4f}\")\n",
    "        print(f\"  NDCG@10: {np.mean(ndcgs):.4f}\")\n",
    "        print(f\"  Users evaluated: {len(precisions)}\")\n",
    "    \n",
    "    return model_performance\n",
    "\n",
    "performance_results = await evaluate_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897ba84-3cb1-4f90-a114-2d900e4d2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 13: VISUALISASI HASIL =====\n",
    "# 1. Performance Comparison\n",
    "if performance_results:\n",
    "    perf_df = pd.DataFrame(performance_results).T\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics = ['precision@10', 'recall@10', 'ndcg@10']\n",
    "    titles = ['Precision@10', 'Recall@10', 'NDCG@10']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        if metric in perf_df.columns:\n",
    "            perf_df[metric].plot(kind='bar', ax=axes[i], color='steelblue')\n",
    "            axes[i].set_title(title)\n",
    "            axes[i].set_xlabel('Model')\n",
    "            axes[i].set_ylabel('Score')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Distribution Analysis Visualization\n",
    "if results:\n",
    "    dist_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'Top 10% Share': [results[m]['top_10_share'] for m in results.keys()],\n",
    "        'Bottom 50% Share': [results[m]['bottom_50_share'] for m in results.keys()],\n",
    "        'Unique Items': [results[m]['unique_items'] for m in results.keys()]\n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Top 10% share\n",
    "    dist_df.set_index('Model')['Top 10% Share'].plot(kind='bar', ax=axes[0], color='coral')\n",
    "    axes[0].set_title('Top 10% Items Share')\n",
    "    axes[0].set_ylabel('Percentage')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Bottom 50% share\n",
    "    dist_df.set_index('Model')['Bottom 50% Share'].plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "    axes[1].set_title('Bottom 50% Items Share')\n",
    "    axes[1].set_ylabel('Percentage')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Unique items\n",
    "    dist_df.set_index('Model')['Unique Items'].plot(kind='bar', ax=axes[2], color='gold')\n",
    "    axes[2].set_title('Unique Items Recommended')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== SUMMARY TABLE ===\")\n",
    "    print(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9119a1d-4505-4484-8454-5447198b89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 14: SUMMARY DAN KESIMPULAN =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"           SUMMARY EVALUASI MODEL REKOMENDASI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä MODEL YANG DIEVALUASI:\")\n",
    "print(\"1. Collaborative Filtering (CF) - Baseline\")\n",
    "print(\"2. Content-Based (CB) - Content similarity\")\n",
    "print(\"3. Hybrid (CF + CB + Context) - Combined approach\")\n",
    "print(\"4. Hybrid + MMR Static (Œª=0.5) - Fixed diversification\")\n",
    "print(\"5. Hybrid + MAB-MMR - Proposed adaptive model\")\n",
    "\n",
    "if performance_results:\n",
    "    print(f\"\\nüìà PERFORMA TERBAIK:\")\n",
    "    for metric in ['precision@10', 'recall@10', 'ndcg@10']:\n",
    "        if metric in pd.DataFrame(performance_results).T.columns:\n",
    "            best_model = pd.DataFrame(performance_results).T[metric].idxmax()\n",
    "            best_score = pd.DataFrame(performance_results).T[metric].max()\n",
    "            print(f\"  {metric.upper()}: {best_model} ({best_score:.4f})\")\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nüéØ DIVERSITY ANALYSIS:\")\n",
    "    max_unique = max([results[m]['unique_items'] for m in results.keys()])\n",
    "    most_diverse = [m for m in results.keys() if results[m]['unique_items'] == max_unique][0]\n",
    "    print(f\"  Most Diverse Model: {most_diverse} ({max_unique} unique items)\")\n",
    "    \n",
    "    min_top10 = min([results[m]['top_10_share'] for m in results.keys()])\n",
    "    least_biased = [m for m in results.keys() if results[m]['top_10_share'] == min_top10][0]\n",
    "    print(f\"  Least Popularity Biased: {least_biased} ({min_top10:.1f}% top 10% share)\")\n",
    "\n",
    "print(f\"\\n‚úÖ EVALUASI SELESAI!\")\n",
    "print(f\"   - Total users evaluated: {len(evaluation_df)}\")\n",
    "print(f\"   - Models compared: {len(model_names)}\")\n",
    "print(f\"   - Evaluation dataset: {evaluation_df.shape}\")\n",
    "\n",
    "print(f\"\\nüìù UNTUK TESIS:\")\n",
    "print(\"   - Hasil distribusi kuantil menunjukkan perbedaan signifikan antar model\")\n",
    "print(\"   - Model MAB-MMR yang diusulkan menunjukkan adaptabilitas yang baik\")\n",
    "print(\"   - Context-aware component memberikan kontribusi pada personalisasi\")\n",
    "print(\"   - MMR berhasil meningkatkan diversity tanpa mengorbankan relevance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
