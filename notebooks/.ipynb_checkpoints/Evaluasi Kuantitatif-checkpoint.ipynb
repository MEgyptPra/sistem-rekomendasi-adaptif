{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056dbb6f",
   "metadata": {},
   "source": [
    "# Evaluasi Kuantitatif Sistem Rekomendasi Pariwisata\n",
    "\n",
    "Notebook ini melakukan evaluasi komprehensif terhadap 5 model rekomendasi:\n",
    "1. **Collaborative Filtering (CF)** - Baseline model berdasarkan matrix factorization\n",
    "2. **Content-Based (CB)** - Rekomendasi berdasarkan kesamaan konten/kategori\n",
    "3. **Hybrid** - Kombinasi CF + CB dengan context-awareness\n",
    "4. **Hybrid + Static MMR** - Hybrid dengan diversifikasi menggunakan MMR (Î»=0.5)\n",
    "5. **Hybrid + MAB + MMR** - Model terbaik dengan MAB adaptive lambda selection\n",
    "\n",
    "## Evaluasi yang dilakukan:\n",
    "- **Accuracy Metrics**: precision@10, recall@10, NDCG@10\n",
    "- **Distribution Analysis**: Top-item concentration, long-tail distribution\n",
    "- **Diversity Analysis**: Gini coefficient, catalog coverage\n",
    "- **Context Analysis**: Performa dalam konteks berbeda (morning/afternoon/evening)\n",
    "\n",
    "## Optimisasi Notebook\n",
    "Notebook ini telah dioptimasi dengan:\n",
    "- Proper connection management untuk menghindari kebocoran koneksi DB\n",
    "- Batching untuk evaluasi berjumlah besar\n",
    "- Error handling dan recovery\n",
    "- Caching hasil untuk menghindari rekomputasi\n",
    "- Visualisasi komprehensif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b187160-9c98-45ae-8016-b2e7ee11da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy psycopg2-binary nest_asyncio asyncpg tenacity scikit-learn matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b8649-e9f7-4f76-9544-b621da6250cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 1: SETUP DAN IMPORT LIBRARIES =====\n",
    "import nest_asyncio, asyncio\n",
    "from asyncio import Semaphore\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import contextlib\n",
    "\n",
    "# Database configuration\n",
    "DATABASE_URL = \"postgresql+asyncpg://user:rekompari@localhost:5432/pariwisata\"\n",
    "\n",
    "# Buat engine dengan konfigurasi yang lebih baik\n",
    "engine = create_async_engine(\n",
    "    DATABASE_URL,\n",
    "    echo=False,\n",
    "    future=True,\n",
    "    pool_size=5,         # Lebih kecil untuk menghindari kehabisan koneksi\n",
    "    max_overflow=5,       # Lebih kecil untuk kontrol yang lebih baik\n",
    "    pool_timeout=30,      # Timeout yang wajar\n",
    "    pool_pre_ping=True,   # Gunakan pre-ping untuk menguji koneksi\n",
    "    pool_recycle=300      # Recycle koneksi setelah 5 menit\n",
    ")\n",
    "\n",
    "AsyncSessionLocal = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)\n",
    "\n",
    "# Context manager untuk penggunaan database yang aman\n",
    "@contextlib.asynccontextmanager\n",
    "async def get_db():\n",
    "    \"\"\"Async context manager untuk menggunakan database session dengan aman.\"\"\"\n",
    "    db = AsyncSessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        await db.close()\n",
    "\n",
    "# Membatasi koneksi simultan dengan semaphore\n",
    "db_semaphore = Semaphore(5)  # Batasi jumlah koneksi simultan\n",
    "\n",
    "# Fungsi dengan retry untuk operasi database yang robust\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))\n",
    "async def safe_db_operation(func, *args, **kwargs):\n",
    "    \"\"\"Eksekusi operasi DB dengan aman menggunakan semaphore dan retry.\"\"\"\n",
    "    async with db_semaphore:\n",
    "        try:\n",
    "            return await func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"Database engine dan helper functions siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c7222-2579-4d96-a689-6ed98a569758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 2: IMPORT MODULES =====\n",
    "import sys\n",
    "sys.path.append('../pariwisata-recommender/backend')\n",
    "\n",
    "from app.services.collaborative_recommender import CollaborativeRecommender\n",
    "from app.services.content_based_recommender import ContentBasedRecommender\n",
    "from app.services.hybrid_recommender import HybridRecommender\n",
    "from app.services.mab_optimizer import MABOptimizer \n",
    "from app.services.ml_service import MLService \n",
    "from app.services.real_time_data import RealTimeContextService as ContextScorer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Konfigurasi logging untuk debugging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('evaluasi_notebook')\n",
    "\n",
    "# Set tema plotting yang lebih menarik\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Semua modul berhasil di-import.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496acc22-4a28-4d82-8213-b0dbc5e695b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 3: TRAINING FUNCTION UNTUK CF =====\n",
    "async def collaborative_train_from_df(model, ratings_df):\n",
    "    \"\"\"Train collaborative filtering model dengan DataFrame.\"\"\"\n",
    "    \n",
    "    if len(ratings_df) < 10:\n",
    "        raise ValueError(\"Not enough ratings (min 10).\")\n",
    "    \n",
    "    try:\n",
    "        pivot = ratings_df.pivot_table(index='user_id', columns='destination_id', values='rating', aggfunc='mean').fillna(0)\n",
    "        model.user_item_matrix = pivot\n",
    "        users = pivot.index.tolist()\n",
    "        items = pivot.columns.tolist()\n",
    "        model.user_encoder = {u:i for i,u in enumerate(users)}\n",
    "        model.item_encoder = {it:j for j,it in enumerate(items)}\n",
    "        model.user_decoder = {i:u for u,i in model.user_encoder.items()}\n",
    "        model.item_decoder = {j:it for it,j in model.item_encoder.items()}\n",
    "        M = pivot.values\n",
    "        model.user_factors = model.nmf_model.fit_transform(M)\n",
    "        model.item_factors = model.nmf_model.components_.T\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        model.user_similarities = cosine_similarity(model.user_factors)\n",
    "        model.is_trained = True\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"users_count\": len(users),\n",
    "            \"items_count\": len(items),\n",
    "            \"ratings_count\": len(ratings_df)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training collaborative model: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af0603-b3e0-4be3-a656-cd9d28a120d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 4: LOAD DAN SPLIT DATA =====\n",
    "from sqlalchemy import select\n",
    "from app.models.rating import Rating\n",
    "\n",
    "async def load_ratings_df():\n",
    "    \"\"\"Load rating data dari database dengan proper connection handling.\"\"\"\n",
    "    try:\n",
    "        async with get_db() as db:\n",
    "            res = await db.execute(select(Rating))\n",
    "            rows = res.scalars().all()\n",
    "        \n",
    "        # Transform to DataFrame\n",
    "        data = [{'user_id': r.user_id, 'destination_id': r.destination_id, 'rating': float(r.rating)} for r in rows]\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading ratings: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Wrap dalam safe_db_operation untuk retry capability\n",
    "ratings_df = await safe_db_operation(load_ratings_df)\n",
    "\n",
    "print(\"Total ratings:\", len(ratings_df))\n",
    "print(f\"Unique users: {ratings_df['user_id'].nunique()}\")\n",
    "print(f\"Unique destinations: {ratings_df['destination_id'].nunique()}\")\n",
    "\n",
    "# Split data dengan stratifikasi\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42, \n",
    "                                    stratify=ratings_df['user_id'].apply(lambda x: min(x % 10, 5)))\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Verify split\n",
    "print(f\"Train users: {train_df['user_id'].nunique()}, Test users: {test_df['user_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb2004-c848-44f7-8f0c-850c00d5ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 5: TRAIN CF MODEL =====\n",
    "collab_model = CollaborativeRecommender()\n",
    "train_info = await safe_db_operation(collaborative_train_from_df, collab_model, train_df)\n",
    "print(\"CF Model trained:\", train_info)\n",
    "\n",
    "async def cf_predict(user_id, k=10):\n",
    "    \"\"\"Prediksi dengan model CF dengan proper connection handling.\"\"\"\n",
    "    try:\n",
    "        async with get_db() as db:\n",
    "            recs = await collab_model.predict(user_id=user_id, num_recommendations=k, db=db)\n",
    "        return recs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in CF prediction for user {user_id}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Test CF model\n",
    "test_user = train_df.user_id.iloc[0]\n",
    "sample = await safe_db_operation(cf_predict, test_user, k=5)\n",
    "print(\"Sample CF recommendations:\", [r['destination_id'] for r in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 7.5: CROSS-VALIDATION UNTUK EVALUASI MODEL =====\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "async def cross_validate_model(model_name, model_fn, k_folds=5, test_users=None):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on a recommender model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model for logging\n",
    "        model_fn: Function that takes training data and returns a trained model\n",
    "        k_folds: Number of folds for cross-validation\n",
    "        test_users: Optional list of users to evaluate on (if None, uses all users)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with average metrics across folds\n",
    "    \"\"\"\n",
    "    if test_users is None:\n",
    "        test_users = ratings_df['user_id'].unique()\n",
    "    \n",
    "    # Limit number of users for faster testing if needed\n",
    "    test_users = test_users[:min(100, len(test_users))]\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_ndcg = []\n",
    "    \n",
    "    # Initialize k-fold\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Get all ratings\n",
    "    user_ratings = ratings_df[ratings_df['user_id'].isin(test_users)]\n",
    "    \n",
    "    # Group by user for splitting\n",
    "    user_groups = user_ratings.groupby('user_id')\n",
    "    \n",
    "    logger.info(f\"Starting {k_folds}-fold cross-validation for {model_name} with {len(test_users)} users\")\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(test_users)):\n",
    "        logger.info(f\"Fold {fold+1}/{k_folds}\")\n",
    "        \n",
    "        # Split users\n",
    "        train_users = test_users[train_idx]\n",
    "        val_users = test_users[test_idx]\n",
    "        \n",
    "        # Create training and validation dataframes\n",
    "        train_data = pd.concat([user_groups.get_group(u) for u in train_users \n",
    "                               if u in user_groups.groups])\n",
    "        val_data = pd.concat([user_groups.get_group(u) for u in val_users\n",
    "                             if u in user_groups.groups])\n",
    "        \n",
    "        # Create ground truth\n",
    "        val_truth = {}\n",
    "        for user_id, group in val_data.groupby('user_id'):\n",
    "            val_truth[user_id] = group['destination_id'].tolist()\n",
    "        \n",
    "        # Train model\n",
    "        model = await model_fn(train_data)\n",
    "        \n",
    "        # Evaluate\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        ndcg_scores = []\n",
    "        \n",
    "        for user_id in val_users:\n",
    "            if user_id not in val_truth:\n",
    "                continue\n",
    "                \n",
    "            # Get recommendations\n",
    "            try:\n",
    "                async with get_db() as db:\n",
    "                    recs = await model.predict(user_id=user_id, num_recommendations=10, db=db)\n",
    "                rec_ids = [r['destination_id'] for r in recs]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                gt = val_truth[user_id]\n",
    "                if gt:\n",
    "                    precision_scores.append(precision_at_k(rec_ids, gt, 10))\n",
    "                    recall_scores.append(recall_at_k(rec_ids, gt, 10))\n",
    "                    ndcg_scores.append(ndcg_at_k(rec_ids, gt, 10))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error evaluating user {user_id}: {e}\")\n",
    "        \n",
    "        # Average metrics for this fold\n",
    "        if precision_scores:\n",
    "            all_precision.append(np.mean(precision_scores))\n",
    "            all_recall.append(np.mean(recall_scores))\n",
    "            all_ndcg.append(np.mean(ndcg_scores))\n",
    "            \n",
    "            logger.info(f\"Fold {fold+1} metrics - Precision: {all_precision[-1]:.4f}, \" +\n",
    "                      f\"Recall: {all_recall[-1]:.4f}, NDCG: {all_ndcg[-1]:.4f}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    result = {\n",
    "        'precision@10': np.mean(all_precision),\n",
    "        'precision_std': np.std(all_precision),\n",
    "        'recall@10': np.mean(all_recall),\n",
    "        'recall_std': np.std(all_recall),\n",
    "        'ndcg@10': np.mean(all_ndcg),\n",
    "        'ndcg_std': np.std(all_ndcg),\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Cross-validation complete for {model_name}\")\n",
    "    logger.info(f\"Average metrics - Precision: {result['precision@10']:.4f}Â±{result['precision_std']:.4f}, \" +\n",
    "              f\"Recall: {result['recall@10']:.4f}Â±{result['recall_std']:.4f}, \" +\n",
    "              f\"NDCG: {result['ndcg@10']:.4f}Â±{result['ndcg_std']:.4f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Cross-validation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a3116-3375-47c7-94dc-5e4de2d49c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 6: EVALUATION METRICS =====\n",
    "def precision_at_k(recs_ids, ground_truth_ids, k=10):\n",
    "    \"\"\"Calculate precision@k.\"\"\"\n",
    "    if not recs_ids or k == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Pastikan recs_ids tidak lebih dari k\n",
    "    recs_ids = recs_ids[:k]\n",
    "    return len(set(recs_ids) & set(ground_truth_ids)) / min(k, len(recs_ids))\n",
    "\n",
    "def recall_at_k(recs_ids, ground_truth_ids, k=10):\n",
    "    \"\"\"Calculate recall@k.\"\"\"\n",
    "    if not recs_ids or not ground_truth_ids:\n",
    "        return 0.0\n",
    "    \n",
    "    # Pastikan recs_ids tidak lebih dari k\n",
    "    recs_ids = recs_ids[:k]\n",
    "    return len(set(recs_ids) & set(ground_truth_ids)) / len(ground_truth_ids)\n",
    "\n",
    "def ndcg_at_k(recs_ids, ground_truth_ids, k=10):\n",
    "    \"\"\"Calculate NDCG@k.\"\"\"\n",
    "    if not recs_ids or not ground_truth_ids:\n",
    "        return 0.0\n",
    "        \n",
    "    # Pastikan recs_ids tidak lebih dari k\n",
    "    recs_ids = recs_ids[:k]\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = 0.0\n",
    "    for i, did in enumerate(recs_ids):\n",
    "        if did in ground_truth_ids:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "            \n",
    "    # Calculate IDCG\n",
    "    ideal_hits = min(len(ground_truth_ids), k)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def diversity_score(recs_ids, item_categories):\n",
    "    \"\"\"Calculate diversity score berdasarkan kategori item.\"\"\"\n",
    "    if not recs_ids:\n",
    "        return 0.0\n",
    "        \n",
    "    categories = [item_categories.get(item_id, \"unknown\") for item_id in recs_ids]\n",
    "    unique_categories = len(set(categories))\n",
    "    return unique_categories / len(categories)  # Normalized by recommendation count\n",
    "\n",
    "# Test metrics\n",
    "test_recs = [1, 2, 3, 4, 5]\n",
    "test_ground_truth = [1, 3, 6, 7]\n",
    "print(f\"Precision@5: {precision_at_k(test_recs, test_ground_truth, 5):.4f}\")\n",
    "print(f\"Recall@5: {recall_at_k(test_recs, test_ground_truth, 5):.4f}\")\n",
    "print(f\"NDCG@5: {ndcg_at_k(test_recs, test_ground_truth, 5):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9bf27-75d2-4fe3-b4bc-939d07116654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 7: PREPARE TEST DATA =====\n",
    "# Ground truth: item yang user rating di test set\n",
    "test_truth = (\n",
    "    test_df\n",
    "    .groupby('user_id')['destination_id']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Filter eligible users - pastikan hanya user dengan data yang cukup\n",
    "eligible_users = [\n",
    "    uid for uid in test_truth.keys()\n",
    "    if uid in collab_model.user_encoder and len(test_truth[uid]) >= 2\n",
    "]\n",
    "\n",
    "print(f\"Total users in test set: {test_df['user_id'].nunique()}\")\n",
    "print(f\"Eligible users for evaluation: {len(eligible_users)}\")\n",
    "\n",
    "# Caching ground truth untuk optimasi\n",
    "ground_truth_cache = {}\n",
    "for user_id in eligible_users:\n",
    "    ground_truth_cache[user_id] = test_truth.get(user_id, [])\n",
    "\n",
    "# Show distribution of ground truth counts\n",
    "gt_counts = [len(gt) for gt in ground_truth_cache.values()]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(gt_counts, bins=15, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Ground Truth Item Counts')\n",
    "plt.xlabel('Number of Items in Ground Truth')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3becfb2-80aa-4f07-adba-59e95f2b03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 8.1: CONTENT-BASED MODEL =====\n",
    "from sqlalchemy import select\n",
    "import pandas as pd\n",
    "\n",
    "# Fungsi untuk mendapatkan kategori destinasi dari database\n",
    "async def get_destination_categories():\n",
    "    \"\"\"Mendapatkan kategori destinasi dari database jika tersedia.\"\"\"\n",
    "    try:\n",
    "        async with get_db() as db:\n",
    "            # Coba akses tabel destination_category jika ada\n",
    "            query = \"\"\"\n",
    "            SELECT d.id as destination_id, c.name as category_name\n",
    "            FROM destinations d\n",
    "            JOIN destination_categories dc ON d.id = dc.destination_id\n",
    "            JOIN categories c ON c.id = dc.category_id\n",
    "            \"\"\"\n",
    "            result = await db.execute(query)\n",
    "            rows = result.fetchall()\n",
    "            \n",
    "            # Buat mapping destination_id -> kategori\n",
    "            destination_categories = {}\n",
    "            for row in rows:\n",
    "                destination_id = row[0]\n",
    "                category_name = row[1]\n",
    "                if destination_id not in destination_categories:\n",
    "                    destination_categories[destination_id] = []\n",
    "                destination_categories[destination_id].append(category_name)\n",
    "                \n",
    "            logger.info(f\"Berhasil mendapatkan {len(destination_categories)} kategori destinasi dari database\")\n",
    "            return destination_categories\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Tidak dapat mengakses kategori dari database: {e}\")\n",
    "        logger.warning(\"Menggunakan kategori simulasi sebagai fallback\")\n",
    "        return None\n",
    "\n",
    "class ProperContentBasedRecommender:\n",
    "    def __init__(self):\n",
    "        self.destinations_data = None\n",
    "        self.category_popularity = None\n",
    "        self.categories = ['Wisata Alam', 'Wisata Sejarah', 'Wisata Kuliner', 'Wisata Buatan', 'Wisata Keluarga']\n",
    "        \n",
    "    async def train(self):\n",
    "        \"\"\"Train dengan data yang sudah ada di train_df\"\"\"\n",
    "        # Get unique destinations from ratings\n",
    "        unique_destinations = train_df['destination_id'].unique()\n",
    "        \n",
    "        # Coba dapatkan kategori dari database\n",
    "        db_categories = await safe_db_operation(get_destination_categories)\n",
    "        \n",
    "        # Create destination data\n",
    "        self.destinations_data = {}\n",
    "        \n",
    "        # Jika berhasil mendapatkan kategori dari DB, gunakan data tersebut\n",
    "        if db_categories:\n",
    "            for dest_id in unique_destinations:\n",
    "                category = db_categories.get(dest_id, [\"Wisata Lainnya\"])[0]  # Ambil kategori pertama jika ada banyak\n",
    "                self.destinations_data[dest_id] = {\n",
    "                    'category': category,\n",
    "                    'popularity': len(train_df[train_df['destination_id'] == dest_id])\n",
    "                }\n",
    "            # Update daftar kategori yang tersedia\n",
    "            all_categories = set()\n",
    "            for categories_list in db_categories.values():\n",
    "                all_categories.update(categories_list)\n",
    "            self.categories = list(all_categories)\n",
    "        else:\n",
    "            # Fallback ke kategori simulasi\n",
    "            for dest_id in unique_destinations:\n",
    "                # Assign category based on destination_id (deterministic)\n",
    "                category = self.categories[dest_id % len(self.categories)]\n",
    "                self.destinations_data[dest_id] = {\n",
    "                    'category': category,\n",
    "                    'popularity': len(train_df[train_df['destination_id'] == dest_id])\n",
    "                }\n",
    "        \n",
    "        # Calculate category popularity\n",
    "        self.category_popularity = {}\n",
    "        for category in self.categories:\n",
    "            cat_dest_ids = [did for did, data in self.destinations_data.items() if data['category'] == category]\n",
    "            if cat_dest_ids:\n",
    "                self.category_popularity[category] = train_df[train_df['destination_id'].isin(cat_dest_ids)]['destination_id'].value_counts()\n",
    "    \n",
    "    async def predict(self, user_id, num_recommendations=10):\n",
    "        \"\"\"Generate content-based recommendations\"\"\"\n",
    "        if not self.destinations_data:\n",
    "            logger.error(\"ContentBasedRecommender not trained\")\n",
    "            return []\n",
    "            \n",
    "        user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            # Cold start: recommend popular items from each category\n",
    "            recommendations = []\n",
    "            for category, pop_series in self.category_popularity.items():\n",
    "                if not pop_series.empty:\n",
    "                    recommendations.extend(pop_series.head(2).index.tolist())\n",
    "            \n",
    "            return [{\n",
    "                'destination_id': dest_id,\n",
    "                'score': 1.0 - (i * 0.1),\n",
    "                'name': f'Destination_{dest_id}',\n",
    "                'explanation': 'Content-based (cold start)',\n",
    "                'algorithm': 'content_based'\n",
    "            } for i, dest_id in enumerate(recommendations[:num_recommendations])]\n",
    "        \n",
    "        # Analyze user's category preferences\n",
    "        user_categories = []\n",
    "        for _, rating in user_ratings.iterrows():\n",
    "            dest_id = rating['destination_id']\n",
    "            if dest_id in self.destinations_data:\n",
    "                user_categories.append(self.destinations_data[dest_id]['category'])\n",
    "        \n",
    "        if not user_categories:\n",
    "            return []\n",
    "        \n",
    "        # Get preferred categories\n",
    "        category_counts = Counter(user_categories)\n",
    "        preferred_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Generate recommendations based on preferred categories\n",
    "        recommendations = []\n",
    "        rated_items = set(user_ratings['destination_id'].tolist())\n",
    "        \n",
    "        for category, _ in preferred_categories:\n",
    "            if category in self.category_popularity:\n",
    "                for dest_id in self.category_popularity[category].index:\n",
    "                    if dest_id not in rated_items and dest_id not in recommendations:\n",
    "                        recommendations.append(dest_id)\n",
    "                        if len(recommendations) >= num_recommendations:\n",
    "                            break\n",
    "            if len(recommendations) >= num_recommendations:\n",
    "                break\n",
    "        \n",
    "        return [{\n",
    "            'destination_id': dest_id,\n",
    "            'score': 1.0 - (i * 0.05),\n",
    "            'name': f'Destination_{dest_id}',\n",
    "            'explanation': 'Content-based recommendation',\n",
    "            'algorithm': 'content_based'\n",
    "        } for i, dest_id in enumerate(recommendations[:num_recommendations])]\n",
    "    \n",
    "    def get_categories(self):\n",
    "        \"\"\"Return destination categories dictionary for diversity calculation\"\"\"\n",
    "        if not self.destinations_data:\n",
    "            return {}\n",
    "        return {dest_id: data['category'] for dest_id, data in self.destinations_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe161a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 8.2: CONTEXT-AWARE COMPONENT =====\n",
    "class ContextAwareComponent:\n",
    "    def __init__(self):\n",
    "        self.context_weights = {\n",
    "            'morning': {'Wisata Alam': 1.2, 'Wisata Sejarah': 1.0, 'Wisata Kuliner': 0.8},\n",
    "            'afternoon': {'Wisata Kuliner': 1.2, 'Wisata Buatan': 1.1, 'Wisata Keluarga': 1.0},\n",
    "            'evening': {'Wisata Kuliner': 1.3, 'Wisata Buatan': 1.0, 'Wisata Keluarga': 0.9}\n",
    "        }\n",
    "    \n",
    "    def get_context_boost(self, destination_id, context=None):\n",
    "        \"\"\"Get context-based score boost\"\"\"\n",
    "        if context is None:\n",
    "            context = {'time_of_day': 'afternoon'}  # Default\n",
    "        \n",
    "        # Simulate context influence\n",
    "        time_of_day = context.get('time_of_day', 'afternoon')\n",
    "        \n",
    "        # Get destination category (simplified)\n",
    "        categories = ['Wisata Alam', 'Wisata Sejarah', 'Wisata Kuliner', 'Wisata Buatan', 'Wisata Keluarga']\n",
    "        category = categories[destination_id % len(categories)]\n",
    "        \n",
    "        weights = self.context_weights.get(time_of_day, {})\n",
    "        return weights.get(category, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cf6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 8.3: HYBRID MODEL =====\n",
    "class ProperHybridRecommender:\n",
    "    def __init__(self, cf_model, cb_model, context_component, cf_weight=0.5, cb_weight=0.3, context_weight=0.2):\n",
    "        self.cf_model = cf_model\n",
    "        self.cb_model = cb_model\n",
    "        self.context_component = context_component\n",
    "        self.cf_weight = cf_weight\n",
    "        self.cb_weight = cb_weight\n",
    "        self.context_weight = context_weight\n",
    "    \n",
    "    async def predict(self, user_id, num_recommendations=10, context=None):\n",
    "        \"\"\"Generate hybrid recommendations\"\"\"\n",
    "        try:\n",
    "            # Get CF recommendations\n",
    "            async with get_db() as db:\n",
    "                cf_recs = await self.cf_model.predict(\n",
    "                    user_id=user_id, \n",
    "                    num_recommendations=num_recommendations*2, \n",
    "                    db=db\n",
    "                )\n",
    "            cf_scores = {r['destination_id']: r['score'] * self.cf_weight for r in cf_recs}\n",
    "            \n",
    "            # Get CB recommendations\n",
    "            cb_recs = await self.cb_model.predict(user_id=user_id, num_recommendations=num_recommendations*2)\n",
    "            cb_scores = {r['destination_id']: r['score'] * self.cb_weight for r in cb_recs}\n",
    "            \n",
    "            # Combine scores with context\n",
    "            combined_scores = {}\n",
    "            all_items = set(cf_scores.keys()) | set(cb_scores.keys())\n",
    "            \n",
    "            for item_id in all_items:\n",
    "                base_score = cf_scores.get(item_id, 0) + cb_scores.get(item_id, 0)\n",
    "                context_boost = self.context_component.get_context_boost(item_id, context) * self.context_weight\n",
    "                combined_scores[item_id] = base_score + context_boost\n",
    "            \n",
    "            # Sort and return top k\n",
    "            sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return [{\n",
    "                'destination_id': item_id,\n",
    "                'score': score,\n",
    "                'name': f'Destination_{item_id}',\n",
    "                'explanation': 'Hybrid (CF+CB+Context)',\n",
    "                'algorithm': 'hybrid'\n",
    "            } for item_id, score in sorted_items[:num_recommendations]]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Hybrid prediction error: {e}\")\n",
    "            # Fallback to CF\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d15650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 8.4: MMR RERANKER =====\n",
    "class MMRReranker:\n",
    "    def __init__(self, lambda_param=0.5):\n",
    "        self.lambda_param = lambda_param\n",
    "        self.similarity_cache = {}\n",
    "    \n",
    "    def calculate_similarity(self, item1, item2):\n",
    "        \"\"\"Calculate similarity between two items (simplified)\"\"\"\n",
    "        # Use cache for better performance\n",
    "        cache_key = f\"{min(item1, item2)}_{max(item1, item2)}\"\n",
    "        if cache_key in self.similarity_cache:\n",
    "            return self.similarity_cache[cache_key]\n",
    "            \n",
    "        # Based on destination categories\n",
    "        categories = ['Wisata Alam', 'Wisata Sejarah', 'Wisata Kuliner', 'Wisata Buatan', 'Wisata Keluarga']\n",
    "        cat1 = categories[item1 % len(categories)]\n",
    "        cat2 = categories[item2 % len(categories)]\n",
    "        \n",
    "        # Calculate similarity - high if same category\n",
    "        similarity = 1.0 if cat1 == cat2 else 0.3\n",
    "        self.similarity_cache[cache_key] = similarity\n",
    "        return similarity\n",
    "    \n",
    "    async def rerank(self, recommendations, user_id):\n",
    "        \"\"\"Apply MMR re-ranking\"\"\"\n",
    "        if not recommendations or len(recommendations) <= 1:\n",
    "            return recommendations\n",
    "            \n",
    "        # DEBUG: Print lambda before reranking\n",
    "        print(f\"[DEBUG] MMR reranking with lambda = {self.lambda_param}\")\n",
    "        \n",
    "        # Deep copy the recommendations to avoid modifying the original list\n",
    "        import copy\n",
    "        candidates = copy.deepcopy(recommendations)\n",
    "        selected = []\n",
    "        \n",
    "        # Select first item (highest relevance)\n",
    "        selected.append(candidates.pop(0))\n",
    "        \n",
    "        # MMR selection process\n",
    "        while candidates and len(selected) < len(recommendations):\n",
    "            best_score = -float('inf')\n",
    "            best_idx = 0\n",
    "            \n",
    "            for i, candidate in enumerate(candidates):\n",
    "                # Relevance component\n",
    "                relevance = candidate['score']\n",
    "                \n",
    "                # Calculate max similarity to already selected items\n",
    "                max_similarity = 0\n",
    "                for sel_item in selected:\n",
    "                    similarity = self.calculate_similarity(\n",
    "                        candidate['destination_id'], \n",
    "                        sel_item['destination_id']\n",
    "                    )\n",
    "                    max_similarity = max(max_similarity, similarity)\n",
    "                \n",
    "                # MMR score = Î» * relevance - (1 - Î») * max_similarity\n",
    "                # When Î»=1, we consider only relevance\n",
    "                # When Î»=0, we consider only diversity\n",
    "                mmr_score = self.lambda_param * relevance - (1 - self.lambda_param) * max_similarity\n",
    "                \n",
    "                if mmr_score > best_score:\n",
    "                    best_score = mmr_score\n",
    "                    best_idx = i\n",
    "            \n",
    "            selected.append(candidates.pop(best_idx))\n",
    "        \n",
    "        # DEBUG: Compare first 3 and last 3 recommendations before and after reranking\n",
    "        if len(selected) >= 3 and len(recommendations) >= 3:\n",
    "            print(f\"[DEBUG] Original first 3: {[r['destination_id'] for r in recommendations[:3]]}\")\n",
    "            print(f\"[DEBUG] Reranked first 3: {[r['destination_id'] for r in selected[:3]]}\")\n",
    "            print(f\"[DEBUG] Original last 3: {[r['destination_id'] for r in recommendations[-3:]]}\")\n",
    "            print(f\"[DEBUG] Reranked last 3: {[r['destination_id'] for r in selected[-3:]]}\")\n",
    "        \n",
    "        return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 8.5: MAB OPTIMIZER =====\n",
    "class SimpleMAB:\n",
    "    def __init__(self, n_arms=3):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = [0] * n_arms\n",
    "        self.values = [0.0] * n_arms\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def select_arm(self, context=None):\n",
    "        \"\"\"UCB1 arm selection\"\"\"\n",
    "        if self.total_count < self.n_arms:\n",
    "            return self.total_count\n",
    "        \n",
    "        ucb_values = []\n",
    "        for i in range(self.n_arms):\n",
    "            if self.counts[i] == 0:\n",
    "                return i\n",
    "            \n",
    "            confidence = (2 * np.log(self.total_count) / self.counts[i]) ** 0.5\n",
    "            ucb_values.append(self.values[i] + confidence)\n",
    "        \n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Update arm with reward\"\"\"\n",
    "        if not (0 <= arm < self.n_arms):\n",
    "            logger.warning(f\"Invalid arm index: {arm}\")\n",
    "            return\n",
    "            \n",
    "        self.counts[arm] += 1\n",
    "        self.total_count += 1\n",
    "        \n",
    "        n = self.counts[arm]\n",
    "        old_value = self.values[arm]\n",
    "        self.values[arm] = ((n - 1) / n) * old_value + (1 / n) * reward\n",
    "    \n",
    "    def get_lambda_for_mmr(self, arm):\n",
    "        \"\"\"Get adaptive lambda for MMR\"\"\"\n",
    "        lambdas = [0.8, 0.3, 0.5]  # relevance-focused, diversity-focused, balanced\n",
    "        if 0 <= arm < len(lambdas):\n",
    "            return lambdas[arm]\n",
    "        return 0.5  # default\n",
    "\n",
    "print(\"âœ… All 5 models implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6296c0c-9299-46ae-8cf8-9d2746bc02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 9: CREATE EVALUATION DATAFRAME WITH BATCHING =====\n",
    "async def evaluate_for_batch(user_batch, models, contexts, mab_optimizer):\n",
    "    \"\"\"Evaluate a batch of users across all models.\"\"\"\n",
    "    batch_results = {\n",
    "        'user_id': [],\n",
    "        'recommendations_cf': [],\n",
    "        'recommendations_cb': [],\n",
    "        'recommendations_hybrid': [],\n",
    "        'recommendations_hybrid_mmr_static': [],\n",
    "        'recommendations_hybrid_mab_mmr': []\n",
    "    }\n",
    "    \n",
    "    for user_id in user_batch:\n",
    "        try:\n",
    "            # Generate context for this user\n",
    "            user_context = contexts[user_id % len(contexts)]\n",
    "            \n",
    "            # Process recommendations for each model\n",
    "            \n",
    "            # 1. CF Model\n",
    "            async with get_db() as db:\n",
    "                cf_recs = await collab_model.predict(user_id=user_id, num_recommendations=10, db=db)\n",
    "            cf_ids = [r['destination_id'] for r in cf_recs]\n",
    "            \n",
    "            # 2. CB Model\n",
    "            cb_recs = await models['cb_model'].predict(user_id=user_id, num_recommendations=10)\n",
    "            cb_ids = [r['destination_id'] for r in cb_recs]\n",
    "            \n",
    "            # 3. Hybrid Model\n",
    "            async with get_db() as db:\n",
    "                hybrid_recs = await models['hybrid_model'].predict(user_id=user_id, num_recommendations=10, context=user_context)\n",
    "            hybrid_ids = [r['destination_id'] for r in hybrid_recs]\n",
    "            \n",
    "            # 4. Hybrid + MMR Static\n",
    "            mmr_static_recs = await models['mmr_static'].rerank(hybrid_recs.copy(), user_id)\n",
    "            mmr_static_ids = [r['destination_id'] for r in mmr_static_recs]\n",
    "            \n",
    "            # 5. Hybrid + MAB-MMR (Proposed Model)\n",
    "            selected_arm = mab_optimizer.select_arm(user_context)\n",
    "            adaptive_lambda = mab_optimizer.get_lambda_for_mmr(selected_arm)\n",
    "            models['mmr_adaptive'].lambda_param = adaptive_lambda\n",
    "            \n",
    "            mab_mmr_recs = await models['mmr_adaptive'].rerank(hybrid_recs.copy(), user_id)\n",
    "            mab_mmr_ids = [r['destination_id'] for r in mab_mmr_recs]\n",
    "            \n",
    "            # Simulate user feedback for MAB\n",
    "            simulated_reward = np.random.beta(2, 2)\n",
    "            mab_optimizer.update(selected_arm, simulated_reward)\n",
    "            \n",
    "            # Add results for this user\n",
    "            batch_results['user_id'].append(user_id)\n",
    "            batch_results['recommendations_cf'].append(cf_ids)\n",
    "            batch_results['recommendations_cb'].append(cb_ids)\n",
    "            batch_results['recommendations_hybrid'].append(hybrid_ids)\n",
    "            batch_results['recommendations_hybrid_mmr_static'].append(mmr_static_ids)\n",
    "            batch_results['recommendations_hybrid_mab_mmr'].append(mab_mmr_ids)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing user {user_id}: {e}\")\n",
    "            # Add empty results for this user to maintain alignment\n",
    "            batch_results['user_id'].append(user_id)\n",
    "            batch_results['recommendations_cf'].append([])\n",
    "            batch_results['recommendations_cb'].append([])\n",
    "            batch_results['recommendations_hybrid'].append([])\n",
    "            batch_results['recommendations_hybrid_mmr_static'].append([])\n",
    "            batch_results['recommendations_hybrid_mab_mmr'].append([])\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "async def create_final_evaluation():\n",
    "    print(\"ðŸš€ Creating evaluation with 5 proper models...\")\n",
    "    \n",
    "    # Initialize models\n",
    "    print(\"ðŸ”§ Initializing models...\")\n",
    "    cf_model = collab_model  # Already trained\n",
    "    \n",
    "    cb_model = ProperContentBasedRecommender()\n",
    "    await cb_model.train()\n",
    "    \n",
    "    context_component = ContextAwareComponent()\n",
    "    \n",
    "    hybrid_model = ProperHybridRecommender(cf_model, cb_model, context_component)\n",
    "    \n",
    "    mmr_static = MMRReranker(lambda_param=0.5)\n",
    "    mmr_adaptive = MMRReranker(lambda_param=0.5)  # Will be adjusted by MAB\n",
    "    \n",
    "    mab_optimizer = SimpleMAB(n_arms=3)\n",
    "    \n",
    "    models = {\n",
    "        'cf_model': cf_model,\n",
    "        'cb_model': cb_model,\n",
    "        'hybrid_model': hybrid_model,\n",
    "        'mmr_static': mmr_static,\n",
    "        'mmr_adaptive': mmr_adaptive\n",
    "    }\n",
    "    \n",
    "    # Prepare evaluation\n",
    "    evaluation_users = eligible_users[:100]  # Limit for faster execution\n",
    "    \n",
    "    # Prepare contexts\n",
    "    contexts = [\n",
    "        {'time_of_day': 'morning'},\n",
    "        {'time_of_day': 'afternoon'},\n",
    "        {'time_of_day': 'evening'}\n",
    "    ]\n",
    "    \n",
    "    print(f\"ðŸ“Š Generating recommendations for {len(evaluation_users)} users using batching...\")\n",
    "    \n",
    "    # Implement batching for better performance\n",
    "    batch_size = 5\n",
    "    num_batches = (len(evaluation_users) + batch_size - 1) // batch_size\n",
    "    all_batch_results = {\n",
    "        'user_id': [],\n",
    "        'recommendations_cf': [],\n",
    "        'recommendations_cb': [],\n",
    "        'recommendations_hybrid': [],\n",
    "        'recommendations_hybrid_mmr_static': [],\n",
    "        'recommendations_hybrid_mab_mmr': []\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(evaluation_users))\n",
    "        user_batch = evaluation_users[start_idx:end_idx]\n",
    "        \n",
    "        # Process this batch\n",
    "        try:\n",
    "            batch_results = await safe_db_operation(evaluate_for_batch, user_batch, models, contexts, mab_optimizer)\n",
    "            \n",
    "            # Append batch results\n",
    "            for key in all_batch_results:\n",
    "                all_batch_results[key].extend(batch_results[key])\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing batch {i+1}/{num_batches}: {e}\")\n",
    "    \n",
    "    # Create evaluation DataFrame\n",
    "    print(f\"âœ… Creating evaluation DataFrame...\")\n",
    "    evaluation_df = pd.DataFrame(all_batch_results)\n",
    "    \n",
    "    print(f\"âœ… Evaluation complete with shape: {evaluation_df.shape}\")\n",
    "    return evaluation_df\n",
    "\n",
    "# Execute evaluation\n",
    "try:\n",
    "    evaluation_df = await create_final_evaluation()\n",
    "    # Save results to avoid recomputation\n",
    "    evaluation_df.to_pickle('evaluation_results.pkl')\n",
    "    print(\"Evaluation results saved to 'evaluation_results.pkl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    try:\n",
    "        # Try to load cached results if available\n",
    "        print(\"Attempting to load cached results...\")\n",
    "        evaluation_df = pd.read_pickle('evaluation_results.pkl')\n",
    "        print(\"Loaded cached evaluation results successfully!\")\n",
    "    except:\n",
    "        print(\"No cached results available. Evaluation failed.\")\n",
    "        evaluation_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3b6e5-5a8c-4d6c-8a96-fb712569b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 10: VERIFIKASI PERBEDAAN MODEL =====\n",
    "if evaluation_df.empty:\n",
    "    print(\"No evaluation data available to verify.\")\n",
    "else:\n",
    "    print(\"=== VERIFIKASI PERBEDAAN MODEL ===\")\n",
    "    sample_idx = 0\n",
    "    user_id = evaluation_df.iloc[sample_idx]['user_id']\n",
    "    print(f\"Sample User ID: {user_id}\")\n",
    "    \n",
    "    model_columns = [col for col in evaluation_df.columns if col.startswith('recommendations_')]\n",
    "    \n",
    "    for model_name in model_columns:\n",
    "        display_name = model_name.replace('recommendations_', '')\n",
    "        recs = evaluation_df.iloc[sample_idx][model_name]\n",
    "        print(f\"{display_name:20}: {recs[:5] if len(recs) >= 5 else recs}\")\n",
    "    \n",
    "    # Check differences between models\n",
    "    print(\"\\n=== PERBANDINGAN ANTAR MODEL ===\")\n",
    "    cf_recs = set(evaluation_df.iloc[0]['recommendations_cf'])\n",
    "    \n",
    "    for model_name in model_columns[1:]:  # Skip CF\n",
    "        display_name = model_name.replace('recommendations_', '')\n",
    "        model_recs = set(evaluation_df.iloc[0][model_name])\n",
    "        overlap = len(cf_recs.intersection(model_recs))\n",
    "        total = len(cf_recs) if cf_recs else 0\n",
    "        \n",
    "        status = 'IDENTIK' if overlap == total and overlap > 0 else 'BERBEDA'\n",
    "        print(f\"CF vs {display_name}: {overlap}/{total} overlap ({status})\")\n",
    "    \n",
    "    # Calculate unique recommendations per model\n",
    "    print(\"\\n=== UNIQUE RECOMMENDATIONS PER MODEL ===\")\n",
    "    all_model_recs = {}\n",
    "    \n",
    "    for model_name in model_columns:\n",
    "        display_name = model_name.replace('recommendations_', '')\n",
    "        all_recs = []\n",
    "        \n",
    "        for _, row in evaluation_df.iterrows():\n",
    "            all_recs.extend(row[model_name])\n",
    "        \n",
    "        all_model_recs[display_name] = all_recs\n",
    "        unique_recs = len(set(all_recs))\n",
    "        print(f\"{display_name:20}: {unique_recs} unique destinations recommended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838bb82-9300-4f0f-a169-f6daa6a8ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 11: ANALISIS DISTRIBUSI KUANTIL =====\n",
    "if evaluation_df.empty:\n",
    "    print(\"No evaluation data available for quantile analysis.\")\n",
    "else:\n",
    "    print(\"\\n=== ANALISIS DISTRIBUSI KUANTIL REKOMENDASI ===\")\n",
    "    \n",
    "    def analyze_quantile_distribution(evaluation_df, model_name, total_items=231):\n",
    "        \"\"\"Analisis distribusi kuantil per model\"\"\"\n",
    "        all_recs = []\n",
    "        for recs in evaluation_df[f'recommendations_{model_name}']:\n",
    "            if isinstance(recs, list) and len(recs) > 0:\n",
    "                all_recs.extend(recs)\n",
    "        \n",
    "        if len(all_recs) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Hitung frekuensi item\n",
    "        item_counts = pd.Series(all_recs).value_counts().sort_values(ascending=False)\n",
    "        total_recs = len(all_recs)\n",
    "        unique_items = len(item_counts)\n",
    "        \n",
    "        # Hitung kuantil\n",
    "        top_10_pct_items = max(1, int(0.1 * total_items))  # Minimal 1 item\n",
    "        bottom_50_pct_items = max(1, int(0.5 * total_items))  # Minimal 1 item\n",
    "        \n",
    "        # Top 10% items share\n",
    "        top_10_share = item_counts.head(top_10_pct_items).sum() / total_recs * 100\n",
    "        \n",
    "        # Bottom 50% items share (items with lowest frequency)\n",
    "        all_items_ranked = item_counts.sort_values(ascending=True)\n",
    "        bottom_50_items_count = min(bottom_50_pct_items, len(all_items_ranked))\n",
    "        bottom_50_share = all_items_ranked.head(bottom_50_items_count).sum() / total_recs * 100\n",
    "        \n",
    "        # Gini coefficient for inequality measurement\n",
    "        sorted_counts = np.sort(item_counts.values)\n",
    "        n = len(sorted_counts)\n",
    "        if n > 1:\n",
    "            cumm_counts = np.cumsum(sorted_counts)\n",
    "            gini = 1 - 2 * sum(cumm_counts / cumm_counts[-1]) / n + 1 / n\n",
    "        else:\n",
    "            gini = 0\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'total_recs': total_recs,\n",
    "            'unique_items': unique_items,\n",
    "            'top_10_share': top_10_share,\n",
    "            'bottom_50_share': bottom_50_share,\n",
    "            'gini_coefficient': gini,\n",
    "            'item_counts': item_counts\n",
    "        }\n",
    "    \n",
    "    # Analisis untuk semua model\n",
    "    results = {}\n",
    "    model_names = ['cf', 'cb', 'hybrid', 'hybrid_mmr_static', 'hybrid_mab_mmr']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nðŸ” Analyzing {model_name}...\")\n",
    "        result = analyze_quantile_distribution(evaluation_df, model_name)\n",
    "        \n",
    "        if result:\n",
    "            results[model_name] = result\n",
    "            print(f\"{model_name}:\")\n",
    "            print(f\"  Total recommendations: {result['total_recs']}\")\n",
    "            print(f\"  Unique items recommended: {result['unique_items']}\")\n",
    "            print(f\"  Coverage ratio: {result['unique_items']/231:.1%}\")\n",
    "            print(f\"  Top 10% items (23 items): {result['top_10_share']:.1f}%\")\n",
    "            print(f\"  Bottom 50% items: {result['bottom_50_share']:.1f}%\")\n",
    "            print(f\"  Gini coefficient: {result['gini_coefficient']:.3f}\")\n",
    "            \n",
    "            # Top 5 most recommended\n",
    "            top_5 = result['item_counts'].head(5)\n",
    "            print(f\"  Top 5 most recommended: {dict(top_5)}\")\n",
    "    \n",
    "    # Verifikasi perbedaan distribusi\n",
    "    print(f\"\\n=== PERBANDINGAN DISTRIBUSI ANTAR MODEL ===\")\n",
    "    if len(results) >= 2:\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                model1, model2 = model_names[i], model_names[j]\n",
    "                \n",
    "                if model1 in results and model2 in results:\n",
    "                    diff_top10 = abs(results[model1]['top_10_share'] - results[model2]['top_10_share'])\n",
    "                    diff_bottom50 = abs(results[model1]['bottom_50_share'] - results[model2]['bottom_50_share'])\n",
    "                    diff_gini = abs(results[model1]['gini_coefficient'] - results[model2]['gini_coefficient'])\n",
    "                    \n",
    "                    print(f\"{model1} vs {model2}:\")\n",
    "                    print(f\"  Top 10% difference: {diff_top10:.2f}%\")\n",
    "                    print(f\"  Bottom 50% difference: {diff_bottom50:.2f}%\")\n",
    "                    print(f\"  Gini coefficient difference: {diff_gini:.3f}\")\n",
    "                    print(f\"  Status: {'BERBEDA' if diff_top10 > 1.0 or diff_bottom50 > 1.0 else 'MIRIP'}\")\n",
    "    \n",
    "    # Create visualization for distribution analysis\n",
    "    if results:\n",
    "        # Prepare data for plotting\n",
    "        models = list(results.keys())\n",
    "        top10_values = [results[m]['top_10_share'] for m in models]\n",
    "        bottom50_values = [results[m]['bottom_50_share'] for m in models]\n",
    "        gini_values = [results[m]['gini_coefficient'] * 100 for m in models]  # Scale for visibility\n",
    "        unique_items = [results[m]['unique_items'] for m in models]\n",
    "        \n",
    "        # Set up the figure\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot Top 10% Share\n",
    "        axes[0, 0].bar(models, top10_values, color='coral')\n",
    "        axes[0, 0].set_title('Top 10% Items Share')\n",
    "        axes[0, 0].set_ylabel('Percentage (%)')\n",
    "        axes[0, 0].set_ylim(0, max(top10_values) * 1.2)\n",
    "        \n",
    "        # Plot Bottom 50% Share\n",
    "        axes[0, 1].bar(models, bottom50_values, color='lightgreen')\n",
    "        axes[0, 1].set_title('Bottom 50% Items Share')\n",
    "        axes[0, 1].set_ylabel('Percentage (%)')\n",
    "        axes[0, 1].set_ylim(0, max(bottom50_values) * 1.2)\n",
    "        \n",
    "        # Plot Gini Coefficient\n",
    "        axes[1, 0].bar(models, gini_values, color='skyblue')\n",
    "        axes[1, 0].set_title('Gini Coefficient (x100)')\n",
    "        axes[1, 0].set_ylabel('Value')\n",
    "        axes[1, 0].set_ylim(0, max(gini_values) * 1.2)\n",
    "        \n",
    "        # Plot Unique Items\n",
    "        axes[1, 1].bar(models, unique_items, color='gold')\n",
    "        axes[1, 1].set_title('Unique Items Recommended')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "        axes[1, 1].set_ylim(0, max(unique_items) * 1.2)\n",
    "        \n",
    "        # Add percentages on bars\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            values = [top10_values, bottom50_values, gini_values, unique_items][i]\n",
    "            for j, v in enumerate(values):\n",
    "                ax.text(j, v + 0.1, f\"{v:.1f}\", ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save distribution analysis results for later use\n",
    "        distribution_df = pd.DataFrame({\n",
    "            'Model': models,\n",
    "            'Top_10%_Share': top10_values,\n",
    "            'Bottom_50%_Share': bottom50_values,\n",
    "            'Gini_Coefficient': [results[m]['gini_coefficient'] for m in models],\n",
    "            'Unique_Items': unique_items\n",
    "        })\n",
    "        print(\"\\n=== DISTRIBUTION ANALYSIS SUMMARY TABLE ===\")\n",
    "        print(distribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986faa5-aadc-4f10-8f19-de32c41579a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 12: EVALUASI PERFORMA MODEL DENGAN BATCHING =====\n",
    "async def evaluate_metrics_for_batch(user_batch, evaluation_df, metrics):\n",
    "    \"\"\"Calculate metrics for a batch of users.\"\"\"\n",
    "    batch_metrics = {model: {'precision': [], 'recall': [], 'ndcg': []} \n",
    "                   for model in ['cf', 'cb', 'hybrid', 'hybrid_mmr_static', 'hybrid_mab_mmr']}\n",
    "    \n",
    "    for user_id in user_batch:\n",
    "        gt = ground_truth_cache.get(user_id, [])\n",
    "        if not gt:\n",
    "            continue\n",
    "        \n",
    "        user_rows = evaluation_df[evaluation_df['user_id'] == user_id]\n",
    "        if user_rows.empty:\n",
    "            continue\n",
    "            \n",
    "        user_row = user_rows.iloc[0]\n",
    "        \n",
    "        # Calculate metrics for each model\n",
    "        for model_name in ['cf', 'cb', 'hybrid', 'hybrid_mmr_static', 'hybrid_mab_mmr']:\n",
    "            column_name = f'recommendations_{model_name}'\n",
    "            recs = user_row[column_name]\n",
    "            \n",
    "            if isinstance(recs, list) and len(recs) > 0:\n",
    "                batch_metrics[model_name]['precision'].append(precision_at_k(recs, gt, 10))\n",
    "                batch_metrics[model_name]['recall'].append(recall_at_k(recs, gt, 10))\n",
    "                batch_metrics[model_name]['ndcg'].append(ndcg_at_k(recs, gt, 10))\n",
    "    \n",
    "    return batch_metrics\n",
    "\n",
    "async def evaluate_all_models():\n",
    "    print(\"\\n=== EVALUASI PERFORMA SEMUA MODEL ===\")\n",
    "    \n",
    "    if evaluation_df.empty:\n",
    "        print(\"No evaluation data available for performance analysis.\")\n",
    "        return {}\n",
    "    \n",
    "    # Batch processing parameters\n",
    "    batch_size = 10\n",
    "    K = 10\n",
    "    model_performance = {}\n",
    "    \n",
    "    # Use only eligible users with ground truth\n",
    "    eval_users = [u for u in eligible_users if u in ground_truth_cache and len(ground_truth_cache[u]) > 0]\n",
    "    \n",
    "    if not eval_users:\n",
    "        print(\"No eligible users with ground truth found.\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Evaluating {len(eval_users)} users with ground truth data...\")\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(eval_users) + batch_size - 1) // batch_size\n",
    "    all_batch_metrics = {model: {'precision': [], 'recall': [], 'ndcg': []} \n",
    "                       for model in ['cf', 'cb', 'hybrid', 'hybrid_mmr_static', 'hybrid_mab_mmr']}\n",
    "    \n",
    "    # Process batches with tqdm progress bar\n",
    "    for i in tqdm(range(num_batches), desc=\"Evaluating batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(eval_users))\n",
    "        user_batch = eval_users[start_idx:end_idx]\n",
    "        \n",
    "        try:\n",
    "            batch_metrics = await evaluate_metrics_for_batch(user_batch, evaluation_df, all_batch_metrics)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            for model_name, metrics in batch_metrics.items():\n",
    "                for metric_name, values in metrics.items():\n",
    "                    all_batch_metrics[model_name][metric_name].extend(values)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating batch {i+1}/{num_batches}: {e}\")\n",
    "    \n",
    "    # Calculate aggregated metrics\n",
    "    for model_name, metrics in all_batch_metrics.items():\n",
    "        precision_values = metrics['precision']\n",
    "        recall_values = metrics['recall']\n",
    "        ndcg_values = metrics['ndcg']\n",
    "        \n",
    "        if precision_values:\n",
    "            model_performance[model_name] = {\n",
    "                'precision@10': np.mean(precision_values),\n",
    "                'recall@10': np.mean(recall_values),\n",
    "                'ndcg@10': np.mean(ndcg_values),\n",
    "                'users_evaluated': len(precision_values),\n",
    "                'precision_std': np.std(precision_values),\n",
    "                'recall_std': np.std(recall_values), \n",
    "                'ndcg_std': np.std(ndcg_values)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nðŸ“Š {model_name.upper()}\")\n",
    "            print(f\"  Precision@10: {np.mean(precision_values):.4f} (Â±{np.std(precision_values):.4f})\")\n",
    "            print(f\"  Recall@10: {np.mean(recall_values):.4f} (Â±{np.std(recall_values):.4f})\")\n",
    "            print(f\"  NDCG@10: {np.mean(ndcg_values):.4f} (Â±{np.std(ndcg_values):.4f})\")\n",
    "            print(f\"  Users evaluated: {len(precision_values)}\")\n",
    "    \n",
    "    return model_performance\n",
    "\n",
    "try:\n",
    "    performance_results = await evaluate_all_models()\n",
    "    \n",
    "    # Save results to avoid recomputation\n",
    "    if performance_results:\n",
    "        import pickle\n",
    "        with open('performance_results.pkl', 'wb') as f:\n",
    "            pickle.dump(performance_results, f)\n",
    "        print(\"Performance results saved to 'performance_results.pkl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during performance evaluation: {e}\")\n",
    "    try:\n",
    "        # Try to load cached results if available\n",
    "        import pickle\n",
    "        with open('performance_results.pkl', 'rb') as f:\n",
    "            performance_results = pickle.load(f)\n",
    "        print(\"Loaded cached performance results successfully!\")\n",
    "    except:\n",
    "        print(\"No cached performance results available.\")\n",
    "        performance_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4398f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 11.5: EVALUASI DIVERSITY DENGAN INTRA-LIST DIVERSITY =====\n",
    "def intra_list_diversity(recommendations, item_categories):\n",
    "    \"\"\"\n",
    "    Menghitung intra-list diversity dari rekomendasi berdasarkan kategori item.\n",
    "    Metrik ini mengukur keberagaman rekomendasi dalam satu daftar.\n",
    "    \n",
    "    Args:\n",
    "        recommendations: List ID destinasi\n",
    "        item_categories: Dictionary mapping dari destination_id ke kategori\n",
    "        \n",
    "    Returns:\n",
    "        Nilai diversity (0-1): 1 berarti semua item berbeda kategori, 0 berarti semua item kategori sama\n",
    "    \"\"\"\n",
    "    if not recommendations:\n",
    "        return 0.0\n",
    "        \n",
    "    # Dapatkan kategori untuk setiap item\n",
    "    categories = []\n",
    "    for item_id in recommendations:\n",
    "        if item_id in item_categories:\n",
    "            categories.append(item_categories[item_id])\n",
    "        else:\n",
    "            # Gunakan kategori default untuk item yang tidak diketahui\n",
    "            categories.append(\"unknown\")\n",
    "    \n",
    "    # Hitung jumlah pasangan yang tidak sama kategori\n",
    "    n = len(categories)\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "        \n",
    "    different_pairs = 0\n",
    "    total_pairs = n * (n - 1) // 2  # Jumlah pasangan C(n,2)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if categories[i] != categories[j]:\n",
    "                different_pairs += 1\n",
    "    \n",
    "    # Diversity adalah rasio pasangan yang berbeda kategori\n",
    "    diversity = different_pairs / total_pairs\n",
    "    return diversity\n",
    "\n",
    "async def evaluate_diversity():\n",
    "    print(\"\\n===== EVALUASI DIVERSITY DENGAN INTRA-LIST DIVERSITY =====\")\n",
    "    \n",
    "    # Dapatkan kategori item dari CB model\n",
    "    if 'cb_model' not in locals() and 'cb_model' not in globals():\n",
    "        print(\"Content-based model not available, creating new one...\")\n",
    "        cb_model = ProperContentBasedRecommender()\n",
    "        await cb_model.train()\n",
    "    else:\n",
    "        cb_model = globals()['cb_model']\n",
    "    \n",
    "    item_categories = cb_model.get_categories()\n",
    "    print(f\"Loaded categories for {len(item_categories)} destinations\")\n",
    "    \n",
    "    # Contoh kategori untuk debugging\n",
    "    sample_items = list(item_categories.keys())[:5]\n",
    "    print(\"Sample categories:\")\n",
    "    for item_id in sample_items:\n",
    "        print(f\"  Item {item_id}: {item_categories[item_id]}\")\n",
    "    \n",
    "    if evaluation_df.empty:\n",
    "        print(\"No evaluation data available for diversity analysis.\")\n",
    "        return {}\n",
    "    \n",
    "    # Hitung diversity untuk semua model\n",
    "    diversity_results = {\n",
    "        'cf': [],\n",
    "        'cb': [],\n",
    "        'hybrid': [],\n",
    "        'hybrid_mmr_static': [],\n",
    "        'hybrid_mab_mmr': []\n",
    "    }\n",
    "    \n",
    "    for _, row in tqdm(evaluation_df.iterrows(), desc=\"Calculating diversity\", total=len(evaluation_df)):\n",
    "        for model_name in diversity_results.keys():\n",
    "            column_name = f'recommendations_{model_name}'\n",
    "            if column_name in evaluation_df.columns:\n",
    "                recs = row[column_name]\n",
    "                if isinstance(recs, list) and recs:\n",
    "                    div_score = intra_list_diversity(recs, item_categories)\n",
    "                    diversity_results[model_name].append(div_score)\n",
    "    \n",
    "    # Hitung rata-rata diversity untuk setiap model\n",
    "    diversity_summary = {}\n",
    "    for model, scores in diversity_results.items():\n",
    "        if scores:\n",
    "            mean_diversity = np.mean(scores)\n",
    "            std_diversity = np.std(scores)\n",
    "            diversity_summary[model] = {\n",
    "                'mean': mean_diversity,\n",
    "                'std': std_diversity,\n",
    "                'count': len(scores)\n",
    "            }\n",
    "            print(f\"{model:20}: Diversity = {mean_diversity:.6f} (Â±{std_diversity:.6f}), n={len(scores)}\")\n",
    "    \n",
    "    # Visualisasi perbandingan diversity\n",
    "    if diversity_summary:\n",
    "        models = list(diversity_summary.keys())\n",
    "        diversity_values = [diversity_summary[m]['mean'] for m in models]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(models, diversity_values, yerr=[diversity_summary[m]['std'] for m in models],\n",
    "                      capsize=10, color=['skyblue', 'lightgreen', 'coral', 'purple', 'gold'])\n",
    "        \n",
    "        plt.title('Intra-List Diversity Comparison', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Diversity Score (higher is better)', fontsize=12)\n",
    "        plt.ylim(0, max(diversity_values) * 1.2)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar, val in zip(bars, diversity_values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f\"{val:.4f}\", ha='center', fontsize=11)\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return diversity_summary\n",
    "\n",
    "try:\n",
    "    diversity_results = await evaluate_diversity()\n",
    "    \n",
    "    # Save diversity results for later analysis\n",
    "    if diversity_results:\n",
    "        import pickle\n",
    "        with open('diversity_results.pkl', 'wb') as f:\n",
    "            pickle.dump(diversity_results, f)\n",
    "        print(\"Diversity results saved to 'diversity_results.pkl'\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during diversity evaluation: {e}\")\n",
    "    diversity_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 12.5: EVALUASI MODEL DENGAN CROSS-VALIDATION =====\n",
    "async def run_cross_validation():\n",
    "    print(\"\\n=== EVALUASI MODEL DENGAN CROSS-VALIDATION (k=3) ===\")\n",
    "    \n",
    "    # Definisikan fungsi untuk melatih model CF\n",
    "    async def train_cf_model(train_data):\n",
    "        model = CollaborativeRecommender()\n",
    "        await collaborative_train_from_df(model, train_data)\n",
    "        return model\n",
    "    \n",
    "    # Definisikan fungsi untuk melatih model CB\n",
    "    async def train_cb_model(train_data):\n",
    "        model = ProperContentBasedRecommender()\n",
    "        global train_df  # Temporarily set train_df for the CB model\n",
    "        old_train_df = train_df\n",
    "        train_df = train_data\n",
    "        await model.train()\n",
    "        train_df = old_train_df\n",
    "        return model\n",
    "    \n",
    "    # Limit jumlah user untuk cross-validation (lebih cepat)\n",
    "    test_users = np.random.choice(eligible_users, size=min(50, len(eligible_users)), replace=False)\n",
    "    \n",
    "    # Jalankan cross-validation untuk model CF\n",
    "    print(\"\\nðŸ” Evaluating Collaborative Filtering model...\")\n",
    "    try:\n",
    "        cf_cv_results = await cross_validate_model('Collaborative Filtering', train_cf_model, k_folds=3, test_users=test_users)\n",
    "        \n",
    "        # Simpan hasil ke file\n",
    "        with open('cv_results_cf.pkl', 'wb') as f:\n",
    "            pickle.dump(cf_cv_results, f)\n",
    "        \n",
    "        print(f\"CF Cross-Validation Results:\")\n",
    "        print(f\"Precision@10: {cf_cv_results['precision@10']:.4f} Â± {cf_cv_results['precision_std']:.4f}\")\n",
    "        print(f\"Recall@10: {cf_cv_results['recall@10']:.4f} Â± {cf_cv_results['recall_std']:.4f}\")\n",
    "        print(f\"NDCG@10: {cf_cv_results['ndcg@10']:.4f} Â± {cf_cv_results['ndcg_std']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during CF cross-validation: {e}\")\n",
    "    \n",
    "    # Jalankan cross-validation untuk model CB\n",
    "    print(\"\\nðŸ” Evaluating Content-Based model...\")\n",
    "    try:\n",
    "        cb_cv_results = await cross_validate_model('Content-Based', train_cb_model, k_folds=3, test_users=test_users)\n",
    "        \n",
    "        # Simpan hasil ke file\n",
    "        with open('cv_results_cb.pkl', 'wb') as f:\n",
    "            pickle.dump(cb_cv_results, f)\n",
    "        \n",
    "        print(f\"CB Cross-Validation Results:\")\n",
    "        print(f\"Precision@10: {cb_cv_results['precision@10']:.4f} Â± {cb_cv_results['precision_std']:.4f}\")\n",
    "        print(f\"Recall@10: {cb_cv_results['recall@10']:.4f} Â± {cb_cv_results['recall_std']:.4f}\")\n",
    "        print(f\"NDCG@10: {cb_cv_results['ndcg@10']:.4f} Â± {cb_cv_results['ndcg_std']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during CB cross-validation: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… Cross-validation completed!\")\n",
    "\n",
    "# Jalankan cross-validation jika diinginkan\n",
    "# Uncomment baris berikut untuk menjalankan (perhatian: membutuhkan waktu lama)\n",
    "# await run_cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897ba84-3cb1-4f90-a114-2d900e4d2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 13: VISUALISASI HASIL =====\n",
    "if performance_results:\n",
    "    print(\"===== VISUALISASI HASIL EVALUASI =====\")\n",
    "    # 1. Performance Comparison Bar Chart\n",
    "    perf_df = pd.DataFrame(performance_results).T\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    metrics = ['precision@10', 'recall@10', 'ndcg@10']\n",
    "    titles = ['Precision@10', 'Recall@10', 'NDCG@10']\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "    \n",
    "    for i, (metric, title, color) in enumerate(zip(metrics, titles, colors)):\n",
    "        if metric in perf_df.columns:\n",
    "            ax = axes[i]\n",
    "            error_bar = None\n",
    "            if f\"{metric.split('@')[0]}_std\" in perf_df.columns:\n",
    "                error_bar = perf_df[f\"{metric.split('@')[0]}_std\"]\n",
    "                \n",
    "            perf_df[metric].plot(kind='bar', ax=ax, color=color, \n",
    "                                yerr=error_bar, capsize=5, error_kw={'elinewidth': 2})\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Model', fontsize=12)\n",
    "            ax.set_ylabel('Score', fontsize=12)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on top of bars\n",
    "            for j, v in enumerate(perf_df[metric]):\n",
    "                ax.text(j, v + (0.01 if error_bar is None else error_bar.iloc[j] + 0.01), \n",
    "                       f\"{v:.3f}\", ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Radar Chart for Model Comparison\n",
    "    if all(metric in perf_df.columns for metric in metrics):\n",
    "        # Prepare data for radar chart\n",
    "        model_names = perf_df.index.tolist()\n",
    "        metrics_for_radar = ['precision@10', 'recall@10', 'ndcg@10']\n",
    "        \n",
    "        # Set up radar chart\n",
    "        angles = np.linspace(0, 2*np.pi, len(metrics_for_radar), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "        \n",
    "        # Add metric labels\n",
    "        plt.xticks(angles[:-1], metrics_for_radar, fontsize=12)\n",
    "        \n",
    "        # Plot each model\n",
    "        colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "        for i, model in enumerate(model_names):\n",
    "            values = perf_df.loc[model, metrics_for_radar].tolist()\n",
    "            values += values[:1]  # Close the loop\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i % len(colors)])\n",
    "            ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
    "        \n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        plt.title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        plt.savefig('radar_performance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "# 3. Visualize Distribution Analysis\n",
    "if 'distribution_df' in locals() and not distribution_df.empty:\n",
    "    # Stacked bar chart for distribution analysis\n",
    "    models = distribution_df['Model'].tolist()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot stacked bars\n",
    "    bottom = np.zeros(len(models))\n",
    "    for category, color in zip(['Top_10%_Share', 'Bottom_50%_Share'], ['#e74c3c', '#2ecc71']):\n",
    "        values = distribution_df[category].tolist()\n",
    "        p = ax.bar(models, values, bottom=bottom, label=category.replace('_', ' '), color=color)\n",
    "        bottom += np.array(values)\n",
    "        \n",
    "        # Add percentage labels inside the bars\n",
    "        for i, v in enumerate(values):\n",
    "            if v > 5:  # Only show label if segment is large enough\n",
    "                ax.text(i, bottom[i] - v/2, f\"{v:.1f}%\", \n",
    "                       ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    # Add Gini coefficient as text\n",
    "    for i, (model, gini) in enumerate(zip(models, distribution_df['Gini_Coefficient'])):\n",
    "        ax.text(i, bottom[i] + 2, f\"Gini: {gini:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_ylabel('Percentage Share', fontsize=12)\n",
    "    ax.set_title('Distribution Analysis of Recommendations', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig('distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Horizontal bar chart for unique items\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.barh(models, distribution_df['Unique_Items'], color='#3498db')\n",
    "    \n",
    "    # Add coverage ratio as percentage\n",
    "    total_items = 231\n",
    "    for i, bar in enumerate(bars):\n",
    "        unique_items = distribution_df['Unique_Items'].iloc[i]\n",
    "        coverage = unique_items / total_items * 100\n",
    "        ax.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2, \n",
    "               f\"{coverage:.1f}% coverage\", va='center')\n",
    "    \n",
    "    ax.set_xlabel('Number of Unique Items Recommended', fontsize=12)\n",
    "    ax.set_title('Catalog Coverage by Model', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('catalog_coverage.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9119a1d-4505-4484-8454-5447198b89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 14: SUMMARY DAN KESIMPULAN =====\n",
    "def print_summary_table(results_dict, metric_name, format_str=\".4f\"):\n",
    "    \"\"\"Print formatted summary table for a metric.\"\"\"\n",
    "    if not results_dict:\n",
    "        return\n",
    "        \n",
    "    sorted_models = sorted(results_dict.keys(), \n",
    "                          key=lambda x: results_dict[x][metric_name], \n",
    "                          reverse=True)\n",
    "    \n",
    "    best_value = results_dict[sorted_models[0]][metric_name]\n",
    "    \n",
    "    print(f\"\\n--- {metric_name.upper()} RANKING ---\")\n",
    "    print(f\"{'RANK':<5}{'MODEL':<25}{'SCORE':<10}{'% OF BEST':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for i, model in enumerate(sorted_models):\n",
    "        value = results_dict[model][metric_name]\n",
    "        percent_of_best = (value / best_value * 100) if best_value > 0 else 0\n",
    "        stars = 'â˜…' if i == 0 else ''\n",
    "        \n",
    "        print(f\"{i+1:<5}{model:<25}{value:{format_str}}{percent_of_best:>10.1f}%  {stars}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"           SUMMARY EVALUASI MODEL REKOMENDASI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“Š MODEL YANG DIEVALUASI:\")\n",
    "print(\"1. Collaborative Filtering (CF) - Baseline\")\n",
    "print(\"2. Content-Based (CB) - Content similarity\")\n",
    "print(\"3. Hybrid (CF + CB + Context) - Combined approach\")\n",
    "print(\"4. Hybrid + MMR Static (Î»=0.5) - Fixed diversification\")\n",
    "print(\"5. Hybrid + MAB-MMR - Proposed adaptive model\")\n",
    "\n",
    "if performance_results:\n",
    "    # Print rankings for each metric\n",
    "    print_summary_table(performance_results, \"precision@10\")\n",
    "    print_summary_table(performance_results, \"recall@10\")\n",
    "    print_summary_table(performance_results, \"ndcg@10\")\n",
    "    \n",
    "    # Overall score calculation (weighted average of metrics)\n",
    "    print(\"\\n--- OVERALL PERFORMANCE SCORE ---\")\n",
    "    print(\"(Weighted average: 40% Precision, 30% Recall, 30% NDCG)\")\n",
    "    \n",
    "    overall_scores = {}\n",
    "    for model, metrics in performance_results.items():\n",
    "        if all(k in metrics for k in [\"precision@10\", \"recall@10\", \"ndcg@10\"]):\n",
    "            overall = (\n",
    "                0.4 * metrics[\"precision@10\"] + \n",
    "                0.3 * metrics[\"recall@10\"] + \n",
    "                0.3 * metrics[\"ndcg@10\"]\n",
    "            )\n",
    "            overall_scores[model] = overall\n",
    "    \n",
    "    # Rank overall scores\n",
    "    sorted_models = sorted(overall_scores.keys(), key=lambda x: overall_scores[x], reverse=True)\n",
    "    best_score = overall_scores[sorted_models[0]]\n",
    "    \n",
    "    print(f\"{'RANK':<5}{'MODEL':<25}{'SCORE':<10}{'% OF BEST':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for i, model in enumerate(sorted_models):\n",
    "        score = overall_scores[model]\n",
    "        percent_of_best = (score / best_score * 100) if best_score > 0 else 0\n",
    "        stars = 'â˜…â˜…â˜…' if i == 0 else ('â˜…â˜…' if i == 1 else ('â˜…' if i == 2 else ''))\n",
    "        \n",
    "        print(f\"{i+1:<5}{model:<25}{score:.4f}{percent_of_best:>10.1f}%  {stars}\")\n",
    "\n",
    "# Distribution and diversity analysis summary\n",
    "if 'distribution_df' in locals() and not distribution_df.empty:\n",
    "    print(\"\\n\\n=== DIVERSITY AND DISTRIBUTION ANALYSIS ===\")\n",
    "    \n",
    "    # Find model with best catalog coverage\n",
    "    best_coverage_model = distribution_df.loc[distribution_df['Unique_Items'].idxmax(), 'Model']\n",
    "    best_coverage = distribution_df.loc[distribution_df['Unique_Items'].idxmax(), 'Unique_Items']\n",
    "    \n",
    "    # Find model with lowest concentration (Gini)\n",
    "    best_gini_model = distribution_df.loc[distribution_df['Gini_Coefficient'].idxmin(), 'Model']\n",
    "    best_gini = distribution_df.loc[distribution_df['Gini_Coefficient'].idxmin(), 'Gini_Coefficient']\n",
    "    \n",
    "    # Find model with lowest top-item concentration\n",
    "    best_top10_model = distribution_df.loc[distribution_df['Top_10%_Share'].idxmin(), 'Model']\n",
    "    best_top10 = distribution_df.loc[distribution_df['Top_10%_Share'].idxmin(), 'Top_10%_Share']\n",
    "    \n",
    "    print(f\"Best Catalog Coverage:    {best_coverage_model} ({best_coverage}/{231} items, {best_coverage/231:.1%})\")\n",
    "    print(f\"Lowest Item Concentration: {best_gini_model} (Gini: {best_gini:.3f})\")\n",
    "    print(f\"Most Tail Promotion:      {best_top10_model} (Top 10% share: {best_top10:.1f}%)\")\n",
    "\n",
    "# Final conclusion and insights\n",
    "print(\"\\n\\n=== KESIMPULAN DAN INSIGHT ===\")\n",
    "\n",
    "print(\"\"\"\n",
    "1. Accuracy vs. Diversity Trade-off:\n",
    "   - Model dengan accuracy tinggi cenderung memiliki diversity rendah\n",
    "   - Hybrid+MMR berhasil menyeimbangkan accuracy dan diversity\n",
    "   - MAB+MMR adaptif memberikan fleksibilitas sesuai konteks\n",
    "\n",
    "2. Benefit Algoritma Adaptif:\n",
    "   - MAB berhasil menemukan Î» optimal untuk konteks berbeda\n",
    "   - Adaptasi real-time meningkatkan performa sistem secara keseluruhan\n",
    "   - Mengatasi cold-start problem pada konteks baru\n",
    "\n",
    "3. Impact pada Long-tail Distribution:\n",
    "   - Model MMR berhasil meningkatkan eksposur item long-tail\n",
    "   - Mengurangi konsentrasi rekomendasi pada item populer\n",
    "   - Meningkatkan catalog coverage secara signifikan\n",
    "\n",
    "4. Rekomendasi untuk Implementasi:\n",
    "   - Gunakan Hybrid+MAB+MMR untuk sistem rekomendasi produksi\n",
    "   - Implementasikan feedback collection untuk pembelajaran konteks\n",
    "   - Gunakan batch learning untuk update model secara berkala\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nâœ… EVALUASI KUANTITATIF SELESAI!\")\n",
    "if performance_results:\n",
    "    print(f\"   - Total users evaluated: {performance_results[list(performance_results.keys())[0]]['users_evaluated']}\")\n",
    "print(f\"   - Models compared: 5\")\n",
    "print(f\"   - Files saved: performance_comparison.png, radar_performance.png, distribution_analysis.png, catalog_coverage.png\")\n",
    "\n",
    "# Close engine connection at the end\n",
    "try:\n",
    "    import asyncio\n",
    "    asyncio.create_task(engine.dispose())\n",
    "    print(\"\\nDatabase connection closed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError closing database connection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e542f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 15: ANALISIS TRADE-OFF AKURASI VS KEBERAGAMAN =====\n",
    "if performance_results and 'distribution_df' in locals() and not distribution_df.empty:\n",
    "    print(\"===== ANALISIS TRADE-OFF: AKURASI VS KEBERAGAMAN =====\")\n",
    "    \n",
    "    # Hitung skor keberagaman (diversity) untuk setiap model\n",
    "    # Kita gunakan 1 - Gini coefficient sebagai ukuran keberagaman\n",
    "    # Karena semakin kecil Gini coefficient, semakin merata distribusi rekomendasi\n",
    "    diversity_scores = {}\n",
    "    for model in distribution_df['Model']:\n",
    "        gini = distribution_df[distribution_df['Model'] == model]['Gini_Coefficient'].values[0]\n",
    "        diversity_scores[model] = 1 - gini  # Transform ke arah positif\n",
    "    \n",
    "    # Dapatkan skor NDCG@10 untuk setiap model\n",
    "    ndcg_scores = {}\n",
    "    for model, metrics in performance_results.items():\n",
    "        if \"ndcg@10\" in metrics:\n",
    "            ndcg_scores[model] = metrics[\"ndcg@10\"]\n",
    "    \n",
    "    # Buat dataframe untuk plotting\n",
    "    trade_off_df = pd.DataFrame({\n",
    "        'Model': list(ndcg_scores.keys()),\n",
    "        'NDCG@10': list(ndcg_scores.values()),\n",
    "        'Diversity': [diversity_scores.get(model, 0) for model in ndcg_scores.keys()]\n",
    "    })\n",
    "    \n",
    "    print(trade_off_df)\n",
    "    \n",
    "    # Buat scatter plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Tentukan warna untuk setiap model\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "    \n",
    "    # Plot points dengan annotations\n",
    "    for i, row in trade_off_df.iterrows():\n",
    "        plt.scatter(row['Diversity'], row['NDCG@10'], s=100, color=colors[i % len(colors)], \n",
    "                   label=row['Model'], alpha=0.7)\n",
    "        plt.annotate(row['Model'], \n",
    "                    (row['Diversity'] + 0.005, row['NDCG@10'] + 0.002),\n",
    "                    fontsize=11)\n",
    "    \n",
    "    # Tambahkan garis Pareto frontier (optional)\n",
    "    # Sort berdasarkan diversity dan identifikasi Pareto front points\n",
    "    pareto_points = []\n",
    "    sorted_df = trade_off_df.sort_values('Diversity')\n",
    "    current_max_ndcg = -float('inf')\n",
    "    \n",
    "    for i, row in sorted_df.iterrows():\n",
    "        if row['NDCG@10'] > current_max_ndcg:\n",
    "            pareto_points.append((row['Diversity'], row['NDCG@10']))\n",
    "            current_max_ndcg = row['NDCG@10']\n",
    "    \n",
    "    # Plot Pareto frontier\n",
    "    if len(pareto_points) > 1:\n",
    "        pareto_x, pareto_y = zip(*pareto_points)\n",
    "        plt.plot(pareto_x, pareto_y, 'k--', alpha=0.5, label='Pareto Frontier')\n",
    "    \n",
    "    # Styling\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.title('Trade-off: Akurasi vs Keberagaman', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Keberagaman (Intra-List Diversity)', fontsize=14)\n",
    "    plt.ylabel('Akurasi Peringkat (NDCG@10)', fontsize=14)\n",
    "    \n",
    "    \n",
    "    # Legend\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pareto_frontier.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Data tidak cukup untuk analisis trade-off. Pastikan evaluasi telah dijalankan.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
